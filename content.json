{"meta":{"title":"灵魂有香气a","subtitle":"","description":"","author":"张帅","url":"https://zabernism.github.io/blog","root":"/blog/"},"pages":[{"title":"分类","date":"2021-08-04T05:59:21.104Z","updated":"2021-08-04T05:59:21.090Z","comments":true,"path":"categories/index.html","permalink":"https://zabernism.github.io/blog/categories/","excerpt":"","text":""},{"title":"","date":"2021-08-04T05:52:56.052Z","updated":"2021-08-04T05:52:56.046Z","comments":false,"path":"artitalk/index.html","permalink":"https://zabernism.github.io/blog/artitalk/","excerpt":"","text":""},{"title":"标签","date":"2021-08-04T05:59:14.931Z","updated":"2021-08-04T05:59:14.917Z","comments":true,"path":"tags/index.html","permalink":"https://zabernism.github.io/blog/tags/","excerpt":"","text":""},{"title":"我的朋友们","date":"2021-08-04T05:51:04.009Z","updated":"2021-08-04T05:51:04.000Z","comments":true,"path":"friends/index.html","permalink":"https://zabernism.github.io/blog/friends/","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 申请友联格式 - title: # 名称 avatar: # 头像 url: # 链接 screenshot: # 截图 keywords: # 关键词 description: # 描述"},{"title":"电影解析","date":"2021-08-02T12:50:09.000Z","updated":"2021-08-02T12:55:05.996Z","comments":true,"path":"movies/index.html","permalink":"https://zabernism.github.io/blog/movies/","excerpt":"","text":""},{"title":"","date":"2021-08-04T06:07:31.409Z","updated":"2021-08-04T06:07:31.404Z","comments":true,"path":"about/index.html","permalink":"https://zabernism.github.io/blog/about/","excerpt":"","text":"所有的成功，都来自于不倦的努力和奔跑；所有幸福，都来自平凡的奋斗和坚持，你无法找到捷径。 努力的原因是希望死后的墓志铭可以有底气刻上：一生努力，一生被爱，想要的都拥有，得不到的都释怀。 生活并无完美，与其让生活带来更多的沮丧与抱怨，不如坚持着一份信念，相信通过努力可以让生活变得更好。"}],"posts":[{"title":"浅谈各种锁机制","slug":"09 操作系统/浅谈各种锁机制","date":"2021-08-18T03:02:42.882Z","updated":"2021-08-18T08:35:18.613Z","comments":true,"path":"2021/08/18/09 操作系统/浅谈各种锁机制/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/18/09%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%85%E8%B0%88%E5%90%84%E7%A7%8D%E9%94%81%E6%9C%BA%E5%88%B6/","excerpt":"","text":"锁 常见的锁主要分为两大类，第一类包括重量级锁、自旋锁、自适应锁在内的悲观锁，第二类包括轻量级锁、自适应轻量级锁在内的乐观锁 悲观锁顾名思义，对数据的更改持悲观态度，认为每次都会并发执行造成冲突，每次都会上锁保护临界区，别人想拿这个数据就会block直到它拿到锁。这样的锁称为悲观锁。数据库中的表锁，行锁，排它锁属于悲观锁，大部分用的锁是悲观锁。 悲观锁好处：保证不会发生并发执行，数据操作效率高。 悲观锁坏处：影响并发量，可能产生死锁。 重量级锁 进入一个同步、线程安全的方法时，是需要先获得这个方法的锁的，退出这个方法时，则会释放锁。如果获取不到这个锁的话，意味着有别的线程在执行这个方法，这时我们就会马上进入阻塞的状态，等待那个持有锁的线程释放锁，然后再把我们从阻塞的状态唤醒，我们再去获取这个方法的锁。 自旋锁 线程从运行态进入阻塞态这个过程，是非常耗时的，因为不仅需要保存线程此时的执行状态，上下文等数据，还涉及到用户态到内核态的转换。当然，把线程从阻塞态唤醒也是一样，也是非常消耗时间的。 自旋锁就是，如果此时拿不到锁，它不马上进入阻塞状态，而是等待一段时间，看看这段时间有没其他人把这锁给释放了。怎么等呢？这个就类似于线程在那里做空循环，如果循环一定的次数还拿不到锁，那么它才会进入阻塞的状态。 自适应自旋锁自适应自旋锁就牛逼了，它不需要我们人为指定循环几次，它自己本身会进行判断要循环几次，而且每个线程可能循环的次数也是不一样的。而之所以这样做，主要是我们觉得，如果一个线程在不久前拿到过这个锁，或者它之前经常拿到过这个锁，那么我们认为它再次拿到锁的几率非常大，所以循环的次数会多一些。 而如果有些线程从来就没有拿到过这个锁，或者说，平时很少拿到，那么我们认为，它再次拿到的概率是比较小的，所以我们就让它循环的次数少一些。因为你在那里做空循环是很消耗 CPU 的。 乐观锁乐观锁对数据的更改持乐观态度，认为临界区不会发生冲突，持有数据更改的版本号，只有数据在提交的时候校验版本号，如果冲突则回滚事务，不冲突则提交数据。 乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候， 这样可以省去了锁的开销，加大了系统的整个吞吐量。 轻量级锁轻量级锁认为，当你在方法里面执行的时候，其实是很少刚好有人也来执行这个方法的，所以，当我们进入一个方法的时候根本就不用加锁，我们只需要做一个标记就可以了，也就是说，我们可以用一个变量来记录此时该方法是否有人在执行。也就是说，如果这个方法没人在执行，当我们进入这个方法的时候，采用CAS机制，把这个方法的状态标记为已经有人在执行，退出这个方法时，在把这个状态改为了没有人在执行了。 显然，比起加锁操作，这个采用CAS来改变状态的操作，花销就小多了。 然而可能会说，没人来竞争的这种想法，那是你说的而已，那如果万一有人来竞争说呢？也就是说，当一个线程来执行一个方法的时候，方法里面已经有人在执行了。 如果真的遇到了竞争，我们就会认为轻量级锁已经不适合了，我们就会把轻量级锁升级为重量级锁了。 所以轻量级锁适合用在那种，很少出现多个线程竞争一个锁的情况，也就是说，适合那种多个线程总是错开时间来获取锁的情况。 偏向锁偏向锁就更加牛逼了，我们已经觉得轻量级锁已经够轻，然而偏向锁更加省事，偏向锁认为，你轻量级锁每次进入一个方法都需要用CAS来改变状态，退出也需要改变，多麻烦。 偏向锁认为，其实对于一个方法，是很少有两个线程来执行的，搞来搞去，其实也就一个线程在执行这个方法而已，相当于单线程的情况，既然是单线程，那就没必要加锁了。 不过毕竟实际情况的多线程，单线程只是自己认为的而已了，所以呢，偏向锁进入一个方法的时候是这样处理的：如果这个方法没有人进来过，那么一个线程首次进入这个方法的时候，会采用CAS机制，把这个方法标记为有人在执行了，和轻量级锁加锁有点类似，并且也会把该线程的 ID 也记录进去，相当于记录了哪个线程在执行。 然后，但这个线程退出这个方法的时候，它不会改变这个方法的状态，而是直接退出来，懒的去改，因为它认为除了自己这个线程之外，其他线程并不会来执行这个方法。 然后当这个线程想要再次进入这个方法的时候，会判断一下这个方法的状态，如果这个方法已经被标记为有人在执行了，并且线程的ID是自己，那么它就直接进入这个方法执行，啥也不用做 你看，多方便，第一次进入需要CAS机制来设置，以后进出就啥也不用干了，直接进入退出。 然而，现实总是残酷的，毕竟实际情况还是多线程，所以万一有其他线程来进入这个方法呢？如果真的出现这种情况，其他线程一看这个方法的ID不是自己，这个时候说明，至少有两个线程要来执行这个方法论，这意味着偏向锁已经不适用了，这个时候就会从偏向锁升级为轻量级锁。 所以呢，偏向锁适用于那种，始终只有一个线程在执行一个方法的情况哦。","categories":[{"name":"09操作系统","slug":"09操作系统","permalink":"https://zabernism.github.io/blog/categories/09%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"01锁","slug":"09操作系统/01锁","permalink":"https://zabernism.github.io/blog/categories/09%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/01%E9%94%81/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://zabernism.github.io/blog/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://zabernism.github.io/blog/tags/%E9%94%81/"}]},{"title":"linux权限命令","slug":"07 DevOps/2 Linux/权限命令","date":"2021-08-17T09:20:35.248Z","updated":"2021-08-17T09:27:25.487Z","comments":true,"path":"2021/08/17/07 DevOps/2 Linux/权限命令/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/17/07%20DevOps/2%20Linux/%E6%9D%83%E9%99%90%E5%91%BD%E4%BB%A4/","excerpt":"","text":"chmodLinux chmod（英文全拼：change mode）命令是控制用户对文件的权限的命令 Linux/Unix 的文件调用权限分为三级 : 文件所有者（Owner）、用户组（Group）、其它用户（Other Users）。 只有文件所有者和超级用户可以修改文件或目录的权限。可以使用绝对模式（八进制数字模式），符号模式指定文件的权限。 八进制语法chmod命令可以使用八进制数来指定权限。文件或目录的权限位是由9个权限位来控制，每三位为一组，它们分别是文件所有者（User）的读、写、执行，用户组（Group）的读、写、执行以及其它用户（Other）的读、写、执行。 # 权限 rwx 二进制 7 读 + 写 + 执行 rwx 111 6 读 + 写 rw- 110 5 读 + 执行 r-x 101 4 只读 r– 100 3 写 + 执行 -wx 011 2 只写 -w- 010 1 只执行 –x 001 0 无 — 000 例如， 765 将这样解释： 所有者的权限用数字表达：属主的那三个权限位的数字加起来的总和。如 rwx ，也就是 4+2+1 ，应该是 7。 用户组的权限用数字表达：属组的那个权限位数字的相加的总和。如 rw- ，也就是 4+2+0 ，应该是 6。 其它用户的权限数字表达：其它用户权限位的数字相加的总和。如 r-x ，也就是 4+0+1 ，应该是 5。","categories":[{"name":"07DevOps","slug":"07DevOps","permalink":"https://zabernism.github.io/blog/categories/07DevOps/"},{"name":"02linux","slug":"07DevOps/02linux","permalink":"https://zabernism.github.io/blog/categories/07DevOps/02linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://zabernism.github.io/blog/tags/linux/"}]},{"title":"快速排序","slug":"02 数据结构与算法/01 数据结构/04快速排序","date":"2021-08-16T08:54:18.419Z","updated":"2021-08-18T06:05:38.681Z","comments":true,"path":"2021/08/16/02 数据结构与算法/01 数据结构/04快速排序/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/16/02%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/01%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/04%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"package com.yj.trade.server.kafka.producer; public class QuickSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;49, 38, 65, 97, 23, 22, 76, 1, 5, 8, 2, 0, -1, 22&#125;; quickSort(arr, 0, arr.length - 1); System.out.println(&quot;排序后:&quot;); for (int i : arr) &#123; System.out.println(i); &#125; &#125; private static void quickSort(int[] arr, int low, int high) &#123; if (low &lt; high) &#123; // 找寻基准数据的正确索引 int index = getIndex(arr, low, high); // 进行迭代对index之前和之后的数组进行相同的操作使整个数组变成有序 quickSort(arr, low, index - 1); quickSort(arr, index + 1, high); &#125; &#125; private static int getIndex(int[] arr, int low, int high) &#123; // 基准数据--&gt; 假设以左侧为基准 int tmp = arr[low]; while (low &lt; high) &#123; // 当队尾的元素大于等于基准数据时,向前挪动high指针 while (low &lt; high &amp;&amp; arr[high] &gt;= tmp) &#123; high--; &#125; // 如果队尾元素小于tmp了,需要将其赋值给low arr[low] = arr[high]; // 当队首元素小于等于tmp时,向前挪动low指针 while (low &lt; high &amp;&amp; arr[low] &lt;= tmp) &#123; low++; &#125; // 当队首元素大于tmp时,需要将其赋值给high arr[high] = arr[low]; &#125; // 跳出循环时low和high相等,此时的low或high就是tmp的正确索引位置 // 由原理部分可以很清楚的知道low位置的值并不是tmp,所以需要将tmp赋值给arr[low] arr[low] = tmp; // 返回tmp的正确位置 return low; &#125; &#125;","categories":[{"name":"02数据结构与算法","slug":"02数据结构与算法","permalink":"https://zabernism.github.io/blog/categories/02%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"01数据结构","slug":"02数据结构与算法/01数据结构","permalink":"https://zabernism.github.io/blog/categories/02%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/01%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://zabernism.github.io/blog/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"二分查找","slug":"02 数据结构与算法/01 数据结构/二分查找","date":"2021-08-10T12:19:41.260Z","updated":"2021-08-23T14:28:21.894Z","comments":true,"path":"2021/08/10/02 数据结构与算法/01 数据结构/二分查找/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/10/02%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/01%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","excerpt":"","text":"在最简单的形式中，二分查找对具有指定左索引和右索引的连续序列进行操作。这就是所谓的查找空间。二分查找维护查找空间的左、右和中间指示符，并比较查找目标或将查找条件应用于集合的中间值；如果条件不满足或值不相等，则清除目标不可能存在的那一半，并在剩下的一半上继续查找，直到成功为止。如果查以空的一半结束，则无法满足条件，并且无法找到目标。 什么是二分查找二分查找是计算机科学中最基本、最有用的算法之一。 它描述了在有序集合中搜索特定值的过程。 二分查找中使用的术语： 目标 Target —— 你要查找的值索引 Index —— 你要查找的当前位置左、右指示符 Left，Right —— 我们用来维持查找空间的指标中间指示符 Mid —— 我们用来应用条件来确定我们应该向左查找还是向右查找的索引 如何识别二分查找？如前所述，二分查找是一种在每次比较之后将查找空间一分为二的算法。每次需要查找集合中的索引或元素时，都应该考虑二分查找。如果集合是无序的，我们可以总是在应用二分查找之前先对其进行排序。 成功的二分查找的 3 个部分二分查找一般由三个主要部分组成： 预处理 —— 如果集合未排序，则进行排序。 二分查找 —— 使用循环或递归在每次比较后将查找空间划分为两半。 后处理 —— 在剩余空间中确定可行的候选者。 二分查找模板 Iint binarySearch(int[] nums, int target)&#123; if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while(left &lt;= right)&#123; // Prevent (left + right) overflow int mid = (right - left) &gt;&gt; 1 + left; if(nums[mid] == target)&#123; return mid; &#125;else if(nums[mid] &lt; target) &#123; left = mid + 1; &#125;else&#123; right = mid - 1; &#125; &#125; // End Condition: left &gt; right return -1; &#125; 关键属性 二分查找的最基础和最基本的形式。查找条件可以在不与元素的两侧进行比较的情况下确定（或使用它周围的特定元素）。不需要后处理，因为每一步中，你都在检查是否找到了元素。如果到达末尾，则知道未找到该元素。 区分语法 初始条件：left = 0, right = length-1终止：left &gt; right向左查找：right = mid-1向右查找：left = mid+1 练习69. x 的平方根 class Solution &#123; public int mySqrt(int x) &#123; int left = 0,right = x,ans = -1; while(left &lt;=right)&#123; int mid = (right -left) / 2 + left; long m2 =(long) mid * mid; if(m2 == x)&#123; return mid; &#125; if(m2 &lt; x)&#123; ans = mid; left = mid + 1; &#125;else&#123; right = mid - 1; &#125; &#125; return ans; &#125; &#125; 第二种题解 class Solution &#123; public int mySqrt(int x) &#123; int left = 0,right = x; while(left &lt;= right)&#123; int mid = (right - left) / 2 + left; long m2 = (long) mid * mid; if (m2 == x)&#123; return mid; &#125; if (m2 &gt; x)&#123; right = mid - 1; &#125; if (m2 &lt; x)&#123; left = mid + 1; &#125; &#125; // 此时left &gt; right return right; &#125; &#125; 二分查找模板 IIint binarySearch(int[] nums, int target)&#123; if(nums == null || nums.length == 0) return -1; int left = 0, right = nums.length; while(left &lt; right)&#123; // Prevent (left + right) overflow int mid = left + (right - left) / 2; if(nums[mid] == target)&#123; return mid; &#125;else if(nums[mid] &lt; target) &#123; left = mid + 1; &#125;else &#123; right = mid; &#125; &#125; // Post-processing: // left != nums.length 用来防止越界 // End Condition: left == right if(left != nums.length &amp;&amp; nums[left] == target) return left; return -1; &#125; 模板 #2 是二分查找的高级模板。它用于查找需要访问数组中当前索引及其直接右邻居索引的元素或条件。 关键属性 一种实现二分查找的高级方法。 查找条件需要访问元素的直接右邻居。 使用元素的右邻居来确定是否满足条件，并决定是向左还是向右。 保证查找空间在每一步中至少有 2 个元素。 需要进行后处理。 当你剩下 1 个元素时，循环 / 递归结束。 需要评估剩余元素是否符合条件。 区分语法 初始条件：left = 0, right = length 终止：left == right 向左查找：right = mid 向右查找：left = mid+1 278. 第一个错误的版本 题目分析： 1、尽量减少API调用次数，可知要使用二分查找 2、由1&lt;=bad&lt;=n&lt;=231 -1结合题目可知，不存在越界问题 3、算法分析： 当mid的值是bad的时候 那么应当去左边找，此时：right = mid 不是的话只能存在右边 left = mid +1 代码 /* The isBadVersion API is defined in the parent class VersionControl. boolean isBadVersion(int version); */ public class Solution extends VersionControl &#123; public int firstBadVersion(int n) &#123; if(n==1)&#123; return 1; &#125; int left = 1,right = n; while(left &lt; right)&#123; int mid = (right - left) / 2 + left; // 当前是错误版本 if(isBadVersion(mid))&#123; right = mid ; &#125;else&#123; left = mid + 1; &#125; &#125; return left; &#125; &#125; 153. 寻找旋转排序数组中的最小值 class Solution &#123; public int findMin(int[] nums) &#123; /** 当left位置的值小于right的值时候，证明此时的区间已经是顺序的，那么最左侧的值最小 */ int left = 0,right = nums.length - 1; while(left&lt;=right)&#123; int mid = (right - left) / 2 + left; if (nums[left] &lt;= nums[right])&#123; return nums[left]; &#125; if(nums[left] &lt;= nums[mid])&#123; // 此时应当去右边找 left =mid + 1; &#125;else&#123; right = mid; &#125; &#125; return nums[left]; &#125; &#125; 二分查找模版IIIint binarySearch(int[] nums, int target) &#123; if (nums == null || nums.length == 0) return -1; int left = 0, right = nums.length - 1; while (left + 1 &lt; right)&#123; // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] == target) &#123; return mid; &#125; else if (nums[mid] &lt; target) &#123; left = mid; &#125; else &#123; right = mid; &#125; &#125; // Post-processing: // End Condition: left + 1 == right if(nums[left] == target) return left; if(nums[right] == target) return right; return -1; &#125; 模板 #3 是二分查找的另一种独特形式。 它用于搜索需要访问当前索引及其在数组中的直接左右邻居索引的元素或条件。 关键属性 实现二分查找的另一种方法。 搜索条件需要访问元素的直接左右邻居。 使用元素的邻居来确定它是向右还是向左。 保证查找空间在每个步骤中至少有 3 个元素。 需要进行后处理。 当剩下 2 个元素时，循环 / 递归结束。 需要评估其余元素是否符合条件。 区分语法 初始条件：left = 0, right = length-1 终止：left + 1 == right 向左查找：right = mid 向右查找：left = mid Leetcode","categories":[{"name":"02数据结构与算法","slug":"02数据结构与算法","permalink":"https://zabernism.github.io/blog/categories/02%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"01数据结构","slug":"02数据结构与算法/01数据结构","permalink":"https://zabernism.github.io/blog/categories/02%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/01%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://zabernism.github.io/blog/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"MySql优化","slug":"04 数据库/01 MySql/MySql优化","date":"2021-08-09T03:51:58.522Z","updated":"2021-08-10T10:49:34.941Z","comments":true,"path":"2021/08/09/04 数据库/01 MySql/MySql优化/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/09/04%20%E6%95%B0%E6%8D%AE%E5%BA%93/01%20MySql/MySql%E4%BC%98%E5%8C%96/","excerpt":"","text":"说起MySQL的查询优化，相信大家积累一堆技巧：不能使用SELECT *、不使用NULL字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？ 我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用 MySQL逻辑架构如果能在头脑中构建一幅MySQL各组件之间如何协同工作的架构图，有助于深入理解MySQL服务器。下图展示了MySQL的逻辑架构图。 MySQL逻辑架构整体分为三层，最上层为客户端层，并非MySQL所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数(比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责MySQL中的数据存储和提取。和Linux下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过API与存储引擎通信，这些API接口屏蔽了不同存储引擎间的差异。 MySQL查询过程我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已 当向MySQL发送一个请求的时候，MySQL到底做了些什么呢？ 客户端/服务端通信协议MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？ MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 任何的查询语句在开始之前都必须经过检查，即使这条SQL语句永远不会命中缓存 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。 Mysql代码 mysql&gt; select * from t_message limit 10; ...省略结果集 mysql&gt; show status like &#39;last_query_cost&#39;; +-----------------+-------------+ | Variable_name | Value | +-----------------+-------------+ | Last_query_cost | 6391.799000 | +-----------------+-------------+ 示例中的结果表示优化器认为大概需要做6391个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照SQL中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找B+Tree索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用Limit时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本MySQL会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于I/O密集型应用，效率会高很多） 随着MySQL的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handlerAPI。查询过程中的每一张表由一个handler实例表示。 实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如改查询影响到的行数以及执行时间等等。 如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 回头总结一下MySQL整个查询执行过程，总的来说分为5个步骤： 客户端向MySQL服务器发送一条查询请求 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 服务器进行SQL解析、预处理、再由优化器生成对应的执行计划 MySQL根据执行计划，调用存储引擎的API来执行查询 将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从3个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的“绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme设计与数据类型优化选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期，比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL。 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用16为存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间。因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。 schema的列不要太多。原因是存储引擎的API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致CPU占用过高。 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇淫技巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引索引是提高MySQL查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的SQL才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。推荐：带你从头到尾捋一遍MySQL索引结构！ 索引相关的数据结构和算法通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为MySQL在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如InnoDB就是使用的B+Tree。 B+Tree中的B是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为5的记录，其大致流程：先找到根，其值为6，大于5，所以查找左子树，找到3，而5大于3，接着找3的右子树，总共找了3次。同样的方法，如果查找值为8的记录，也需要查找3次。所以二叉查找树的平均查找次数为(3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3次，而顺序查找的话，查找值为2的记录，仅需要1次，但查找值为8的记录则需要6次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3次，因为大多数情况下二叉查找树的平均查找速度比顺序查找要快。 由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值9的节点时，就需要做出如下变动。 通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么MySQL索引不直接使用平衡二叉树？ 随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的I/O读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的I/O存取次数？ 一种行之有效的解决方法是减少树的深度，将二叉树变为m叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为2的简化了的B+Tree。 怎么理解这两个特征？MySQL将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于20而小于30的记录，只需要找到节点20，就可以遍历指针依次找到25、30。如果没有链接指针的话，就无法进行区间查找。这也是MySQL使用B+Tree作为索引存储结构的重要原因。 MySQL为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 页是计算机管理存储器的逻辑块，硬件及OS往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多OS中，页的大小通常为4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 MySQL巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次I/O。假设B+Tree的高度为h，一次检索最多需要h-1I/O（根节点常驻内存），复杂度O(h)=O(logMN)。实际应用场景中，M通常较大，常常超过100，因此树的高度一般都比较小，通常不超过3。 最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储4个内节点。首先要插入第一个节点28，如下图所示。 接着插入下一个节点70，在Index Page中查询后得知应该插入到50 - 70之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为50，所以根据中间值来拆分叶子节点，如下图所示。 最后插入一个节点95，这时候Index Page和Leaf Page都满了，就需要做两次拆分，如下图所示。 Leaf Page与Index Page拆分拆分后最终形成了这样一颗树。 最终树B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要I/O操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当LeafPage已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入70的时候，并不会去做页拆分，而是左旋操作。 左旋操作通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的I/O操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类型，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但MySQL中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表： Mysql代码 CREATE TABLE People( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(`m`,`f`) not null, key(last_name,first_name,dob) ); 对于表中每一行数据，索引中包含了last_name、first_name、dob列的值，下图展示了索引是如何组织数据存储的。 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的“最左原则”。 1、MySQL不会使用索引的情况：非独立的列“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如： select * from where id + 1 = 5 我们很容易看出其等价于 id = 4，但是MySQL无法自动解析这个表达式，使用函数是同样的道理。 2、前缀索引如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 3、多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询： select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 老版本的MySQL会随机选择一个索引，但新版本做如下的优化： select film_id,actor_id from film_actor where actor_id = 1 union all select film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1 当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此explain时如果发现有索引合并（Extra字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让MySQL在查询时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如： SELECT * FROM payment where staff_id = 2 and customer_id = 584 是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近1就把哪个字段索引前面就好。 select count(distinct staff_id)/count(*) as staff_id_selectivity, count(distinct customer_id)/count(*) as customer_id_selectivity, count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息： select user_id from trade where user_group_id = 1 and trade_amount &gt; 0 MySQL为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 4、避免多个范围条件实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户： select user.* from user where login_time &gt; &#39;2017-04-01&#39; and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。 5、覆盖索引如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量 索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多 6、使用索引扫描来排序MySQL有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果explain的结果中type列的值为index表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。 -- 最左列为常数，索引：(date,staff_id,customer_id) select staff_id,customer_id from demo where date = &#39;2015-06-01&#39; ``order by staff_id,customer_id 7、冗余和重复索引冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 8、删除长期未使用的索引定期删除一些长时间未使用过的索引是一个非常好的习惯。 关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，explain后再提测是一种美德。 特定类型查询优化优化COUNT()查询COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN出来的行数就是一个不错的近似值，而且执行EXPLAIN并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用redis这样的外部缓存系统。 优化关联查询在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询： SELECT A.xx,B.yy FROM A INNER JOIN B USING(c) WHERE A.xx IN (5,6) 假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询： outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6); outer_row = outer_iterator.next; while(outer_row) &#123; inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c; inner_row = inner_iterator.next; while(inner_row) &#123; output[inner_row.yy,outer_row.xx]; inner_row = inner_iterator.next; &#125; outer_row = outer_iterator.next; &#125; 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化LIMIT分页当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询： SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子： SELECT film.film_id,film.description FROM film INNER JOIN ( SELECT film_id FROM film ORDER BY title LIMIT 50,5 ) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让MySQL扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询： SELECT id FROM t LIMIT 10000, 10; 改为： SELECT id FROM t WHERE id &gt; 10000 LIMIT 10; 其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化UNIONMySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用ALL关键字，MySQL总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 转载","categories":[{"name":"04数据库","slug":"04数据库","permalink":"https://zabernism.github.io/blog/categories/04%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"01MySql","slug":"04数据库/01MySql","permalink":"https://zabernism.github.io/blog/categories/04%E6%95%B0%E6%8D%AE%E5%BA%93/01MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"https://zabernism.github.io/blog/tags/MySql/"}]},{"title":"文件对象","slug":"01java/07 IO操作/01 文件对象","date":"2021-08-07T09:49:26.347Z","updated":"2021-08-11T11:21:23.433Z","comments":true,"path":"2021/08/07/01java/07 IO操作/01 文件对象/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/07/01java/07%20IO%E6%93%8D%E4%BD%9C/01%20%E6%96%87%E4%BB%B6%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"","categories":[{"name":"01java","slug":"01java","permalink":"https://zabernism.github.io/blog/categories/01java/"},{"name":"07文件对象","slug":"01java/07文件对象","permalink":"https://zabernism.github.io/blog/categories/01java/07%E6%96%87%E4%BB%B6%E5%AF%B9%E8%B1%A1/"}],"tags":[{"name":"java","slug":"java","permalink":"https://zabernism.github.io/blog/tags/java/"}]},{"title":"java多线程","slug":"01java/08 多线程与并发/01 java多线程技能","date":"2021-08-07T07:38:30.022Z","updated":"2021-08-07T09:44:34.251Z","comments":true,"path":"2021/08/07/01java/08 多线程与并发/01 java多线程技能/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/07/01java/08%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/01%20java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%8A%80%E8%83%BD/","excerpt":"","text":"主要讲解线程的启动，如何使用线程暂停，如何使用线程停止，线程的优先级，线程的安全问题等。 java多线程进程和线程进程 进程是一个具有一定独立功能的程序在一个数据集上的一次动态执行的过程，是操作系统进行资源分配和调度的一个独立单位，是应用程序运行的载体。进程是一种抽象的概念，从来没有统一的标准定义。进程一般由程序，数据集合和进程控制块三部分组成。程序用于描述进程要完成的功能，是控制进程执行的指令集；数据集合是程序在执行时所需要的数据和工作区；程序控制块包含进程的描述信息和控制信息是进程存在的唯一标志 进程具有的特征： 动态性：进程是程序的一次执行过程，是临时的，有生命期的，是动态产生，动态消亡的； 并发性：任何进程都可以同其他进行一起并发执行； 独立性：进程是系统进行资源分配和调度的一个独立单位； 结构性：进程由程序，数据和进程控制块三部分组成 初看这段文字可能觉得抽象，那么进程到底是什么东西呢？ 如上图所示，每个操作系统中运行的程序，就对应一个进程（Window电脑中任务管理器同样能看见类似的内容）；进程是受操作系统管理的基本运行单元。 线程 线程可以理解成是在进程中独立运行的子任务，比如qq在运行时候会有很多子任务在同时运行。 为什么要使用多线程？ 如图所示，单线程环境下，如果想要运行多个任务，那么只能等一个任务结束之后才能运行第二个线程，而在多线程环境，可以同时运行任务1和任务2，大大缩短了任务执行的时间，从而提高了CPU的利用率 举个例子：比如打LOL时候，如果是单线程环境那么就会形成以下结果 定义一个主类： public class TestThread &#123; public static void main(String[] args) &#123; Hero galen = new Hero(&quot;盖伦&quot;, 616, 50); Hero timo = new Hero(&quot;提莫&quot;, 300, 30); Hero hunter = new Hero(&quot;赏金猎人&quot;, 500, 65); Hero monk = new Hero(&quot;盲僧&quot;, 455, 80); //盖伦攻击提莫 while (!timo.isDead()) &#123; galen.attackHero(timo); &#125; //赏金猎人攻击盲僧 while (!monk.isDead()) &#123; hunter.attackHero(monk); &#125; &#125; &#125; 定义英雄类 public class Hero &#123; private String name; private float hp; private int damage; public Hero(String name, float hp, int damage) &#123; this.name = name; this.hp = hp; this.damage = damage; &#125; public void attackHero(Hero h) &#123; try &#123; //为了表示攻击需要时间，每次攻击暂停1000毫秒 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; h.hp -= damage; System.out.format(&quot;%s 正在攻击 %s, %s的血变成了 %.0f%n&quot;, name, h.name, h.name, h.hp); if (h.isDead()) &#123; System.out.println(h.name + &quot;死了！&quot;); &#125; &#125; public boolean isDead() &#123; return 0 &gt;= hp; &#125; &#125; 当执行main方法时候会形成一个什么情况呢？只有提莫死了，赏金猎人才开始攻击盲僧。 执行结果： 盖伦 正在攻击 提莫, 提莫的血变成了 250 盖伦 正在攻击 提莫, 提莫的血变成了 200 盖伦 正在攻击 提莫, 提莫的血变成了 150 盖伦 正在攻击 提莫, 提莫的血变成了 100 盖伦 正在攻击 提莫, 提莫的血变成了 50 盖伦 正在攻击 提莫, 提莫的血变成了 0 提莫死了！ 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 390 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 325 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 260 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 195 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 130 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 65 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 0 盲僧死了！ 而在实际过程中，这显然是不合理的，此时就需要多线程的加入。 使用多线程 一个进程中至少会有一个线程在运行 public class Main &#123; public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125; 结果 main 在java中如果调用一个main方法，那么jvm就会相应的创建一个线程，这个线程的名字叫main 继承Thread类新建一个KillThread类用来继承Thread public class KillThread extends Thread&#123; private Hero h1; private Hero h2; public KillThread(Hero h1, Hero h2)&#123; this.h1 = h1; this.h2 = h2; &#125; @Override public void run()&#123; while(!h2.isDead())&#123; h1.attackHero(h2); &#125; &#125; &#125; 对主类的调用进行改造 public class TestThread &#123; public static void main(String[] args) &#123; Hero galen = new Hero(&quot;盖伦&quot;, 616, 50); Hero timo = new Hero(&quot;提莫&quot;, 300, 30); Hero hunter = new Hero(&quot;赏金猎人&quot;, 500, 65); Hero monk = new Hero(&quot;盲僧&quot;, 455, 80); KillThread killThread1 = new KillThread(galen, timo); killThread1.start(); KillThread killThread2 = new KillThread(hunter, monk); killThread2.start(); &#125; &#125; 结果： 盖伦 正在攻击 提莫, 提莫的血变成了 250 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 390 盖伦 正在攻击 提莫, 提莫的血变成了 200 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 325 盖伦 正在攻击 提莫, 提莫的血变成了 150 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 260 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 195 盖伦 正在攻击 提莫, 提莫的血变成了 100 盖伦 正在攻击 提莫, 提莫的血变成了 50 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 130 盖伦 正在攻击 提莫, 提莫的血变成了 0 提莫死了！ 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 65 赏金猎人 正在攻击 盲僧, 盲僧的血变成了 0 盲僧死了！ 值得注意的是： 在使用多线程时候，只能调用类的start方法进行实现，直接调用run方法的话是无法让多线程生效的 实现Runnable接口public class Thread implements Runnable 从上述源码中可以看出来，Thread 类也是实现了Runnable 接口。 此时需要对KillThread 类进行改造 public class KillThread implements Runnable&#123; private Hero h1; private Hero h2; public KillThread(Hero h1, Hero h2)&#123; this.h1 = h1; this.h2 = h2; &#125; @Override public void run()&#123; while(!h2.isDead())&#123; h1.attackHero(h2); &#125; &#125; &#125; 主类进行改造 public class TestThread &#123; public static void main(String[] args) &#123; Hero galen = new Hero(&quot;盖伦&quot;, 616, 50); Hero timo = new Hero(&quot;提莫&quot;, 300, 30); Hero hunter = new Hero(&quot;赏金猎人&quot;, 500, 65); Hero monk = new Hero(&quot;盲僧&quot;, 455, 80); KillThread killThread1 = new KillThread(galen, timo); new Thread(killThread1).start(); KillThread killThread2 = new KillThread(hunter, monk); new Thread(killThread2).start(); &#125; &#125; 此时的执行结果与Thread类的方式基本相同 除此之外还可以使用匿名内部类方式实现 public class TestThread &#123; public static void main(String[] args) &#123; Hero galen = new Hero(&quot;盖伦&quot;, 616, 50); Hero timo = new Hero(&quot;提莫&quot;, 300, 30); Hero hunter = new Hero(&quot;赏金猎人&quot;, 500, 65); Hero monk = new Hero(&quot;盲僧&quot;, 455, 80); Thread t1 = new Thread(() -&gt; &#123; //匿名类中用到外部的局部变量timo，必须把timo声明为final //但是在JDK7以后，就不是必须加final的了 while (!timo.isDead()) &#123; galen.attackHero(timo); &#125; &#125;); t1.start(); Thread t2 = new Thread(() -&gt; &#123; while (!hunter.isDead()) &#123; monk.attackHero(hunter); &#125; &#125;); t2.start(); &#125; &#125;","categories":[{"name":"01java","slug":"01java","permalink":"https://zabernism.github.io/blog/categories/01java/"},{"name":"08多线程与并发","slug":"01java/08多线程与并发","permalink":"https://zabernism.github.io/blog/categories/01java/08%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"java","slug":"java","permalink":"https://zabernism.github.io/blog/tags/java/"}]},{"title":"02Optional","slug":"01java/09 Stream/Optional","date":"2021-08-06T05:57:41.523Z","updated":"2021-08-11T11:55:02.860Z","comments":true,"path":"2021/08/06/01java/09 Stream/Optional/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/06/01java/09%20Stream/Optional/","excerpt":"","text":"Optional@since 1.8 为了解决空指针异常，受到 Google Guava 的启发，Optional 类已经成为 Java 8 类库的一部分。 Optional 实际上是个容器：它可以保存类型T的值，或者仅仅保存 null。Optional 提供很多有用的方法，这样就不用显式进行空值检测。 1 初始化 Optional.ofNullable ： 允许传递为 null 参数 Optional.of ： 如果传递的参数是 null，抛出异常 NullPointerException 2 判断 Optional.isPresent ： 判断值是否存在。非空true，否则false 3 若存在 Optional.get - 如果值存在，返回它，否则抛出异常 Optional.ifPresent： 如果值存在则使用该值调用 consumer , 否则不做任何事情。 Optional.filter ： 如果值存在，并匹配给定的 predicate，返回一个Optional用以描述这个值，否则返回一个空的Optional。 Optional.map ： 如果有值，则对其执行调用映射函数得到返回值。如果返回值不为 null，则创建包含映射返回值的Optional作为map方法返回值，否则返回空Optional。 4 若不存在 Optional.orElse ： 如果值存在，返回它，否则返回默认值 Optional.orElseGet ： 如果值存在，返回它，否则返执行回调函数 Optional.orElseThrow ： 如果值存在，返回它，否则抛出指定异常 Optional&lt; String &gt; fullName = Optional.ofNullable( null ); fullName.isPresent();//false fullName.orElseGet( () -&gt; &quot;[none]&quot; );//[none] fullName.map( s -&gt; &quot;Hey &quot; + s + &quot;!&quot; ).orElse( &quot;Hey Stranger!&quot; );//Hey Stranger! // Returns maximum value in collection as an Optional&lt;E&gt; public static &lt;E extends Comparable&lt;E&gt;&gt; Optional&lt;E&gt; max(Collection&lt;E&gt; c) &#123; if (c.isEmpty()) return Optional.empty(); E result = null; for (E e : c) if (result == null || e.compareTo(result) &gt; 0) result = Objects.requireNonNull(e); return Optional.of(result); &#125; // Using an optional to provide a chosen default value String lastWordInLexicon = max(words).orElse(&quot;No words...&quot;); // Using an optional to throw a chosen exception Toy myToy = max(toys).orElseThrow(TemperTantrumException::new); // Using optional when you know there’s a return value Element lastNobleGas = max(Elements.NOBLE_GASES).get();","categories":[{"name":"01java","slug":"01java","permalink":"https://zabernism.github.io/blog/categories/01java/"},{"name":"09Stream","slug":"01java/09Stream","permalink":"https://zabernism.github.io/blog/categories/01java/09Stream/"}],"tags":[{"name":"java","slug":"java","permalink":"https://zabernism.github.io/blog/tags/java/"}]},{"title":"分布式主键生成器～雪花算法","slug":"06 辅助工具类/分布式主键生成器～雪花算法","date":"2021-08-06T05:50:00.427Z","updated":"2021-08-06T05:51:25.953Z","comments":true,"path":"2021/08/06/06 辅助工具类/分布式主键生成器～雪花算法/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/06/06%20%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%E7%B1%BB/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%BB%E9%94%AE%E7%94%9F%E6%88%90%E5%99%A8%EF%BD%9E%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/","excerpt":"","text":"@Slf4j public class SnowFlakeUtil &#123; private static SnowFlakeUtil flowIdWorker = null; private final long id; /** * 时间起始标记点，作为基准，一般取系统的最近时间 */ private final Long epoch = 1524291141010L; /** * 机器标识位数 */ private final long workerIdBits = 10L; /** * 机器ID最大值: 1023 */ private final long maxWorkerId = -1L ^ -1L &lt;&lt; this.workerIdBits; /** * 毫秒内自增位 */ private final long sequenceBits = 12L; /** * 12 */ private final long workerIdShift = this.sequenceBits; /** * 22 */ private final long timestampLeftShift = this.sequenceBits + this.workerIdBits; /** * 4095,111111111111,12位 */ private final long sequenceMask = -1L ^ -1L &lt;&lt; this.sequenceBits; /** * 0，并发控制 */ private long sequence = 0L; private long lastTimestamp = -1L; private SnowFlakeUtil(Long id) &#123; if (id &gt; this.maxWorkerId || id &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;Worker Id can&#39;t be greater than %d or less than 0&quot;, this.maxWorkerId)); &#125; this.id = id; &#125; public static SnowFlakeUtil getFlowIdInstance() &#123; if (null == flowIdWorker) &#123; synchronized (SnowFlakeUtil.class) &#123; if (null == flowIdWorker) &#123; flowIdWorker = new SnowFlakeUtil(1L); &#125; &#125; &#125; return flowIdWorker; &#125; /** * 获得系统当前毫秒数 */ private static Long timeGen() &#123; return System.currentTimeMillis(); &#125; public synchronized Long nextId() &#123; long timestamp = timeGen(); if (this.lastTimestamp == timestamp) &#123; //如果上一个timestamp与新产生的相等，则sequence加一(0-4095循环); 对新的timestamp，sequence从0开始 this.sequence = this.sequence + 1 &amp; this.sequenceMask; if (this.sequence == 0) &#123; // 重新生成timestamp timestamp = this.tilNextMillis(this.lastTimestamp); &#125; &#125; else &#123; this.sequence = 0; &#125; if (timestamp &lt; this.lastTimestamp) &#123; log.error(String.format(&quot;Clock moved backwards.Refusing to generate id for %d milliseconds&quot;, (this.lastTimestamp - timestamp))); return -1L; &#125; this.lastTimestamp = timestamp; return timestamp - this.epoch &lt;&lt; this.timestampLeftShift | this.id &lt;&lt; this.workerIdShift | this.sequence; &#125; /** * 等待下一个毫秒的到来, 保证返回的毫秒数在参数lastTimestamp之后 */ private Long tilNextMillis(Long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; &#125;","categories":[{"name":"06辅助工具类","slug":"06辅助工具类","permalink":"https://zabernism.github.io/blog/categories/06%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%E7%B1%BB/"}],"tags":[{"name":"雪花算法","slug":"雪花算法","permalink":"https://zabernism.github.io/blog/tags/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"}]},{"title":"二十个实例轻松搞定Stream流库","slug":"01java/09 Stream/20个实例轻松搞定Stream流库","date":"2021-08-03T05:06:52.979Z","updated":"2021-08-11T11:59:25.662Z","comments":true,"path":"2021/08/03/01java/09 Stream/20个实例轻松搞定Stream流库/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/03/01java/09%20Stream/20%E4%B8%AA%E5%AE%9E%E4%BE%8B%E8%BD%BB%E6%9D%BE%E6%90%9E%E5%AE%9AStream%E6%B5%81%E5%BA%93/","excerpt":"","text":"20个实例轻松搞定Stream流库概述Java 8 是一个非常成功的版本，这个版本新增的Stream，配合同版本出现的 Lambda，给我们操作集合（Collection）提供了极大的便利。 那么什么是Stream？ Stream将要处理的元素集合看作一种流，在流的过程中，借助Stream API对流中的元素进行操作，比如：筛选、排序、聚合等。 Stream可以由数组或集合创建，对流的操作分为两种： 中间操作，每次返回一个新的流，可以有多个。 终端操作，每个流只能进行一次终端操作，终端操作结束后流无法再次使用。终端操作会产生一个新的集合或值。 另外，Stream有几个特性： stream不存储数据，而是按照特定的规则对数据进行计算，一般会输出结果。 stream不会改变数据源，通常情况下会产生一个新的集合或一个值。 stream具有延迟执行特性，只有调用终端操作时，中间操作才会执行 Stream的创建Stream可以通过集合数组创建 通过集合数组创建List&lt;String&gt; aList = Arrays.asList(&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;); // 创建一个顺序流 Stream&lt;String&gt; stream = aList.stream(); // 创建一个并行流 Stream&lt;String&gt; parallelStream = aList.parallelStream(); 通过数组创建int[] aArray = &#123;1, 3, 5, 7, 9&#125;; IntStream stream = Arrays.stream(aArray); 通过Stream的静态方法进行创建// 通过of方法创建 Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3); stream.forEach(System.out::println); System.out.println(&quot;---------------------------&quot;); // 通过iterate方法创建 Stream&lt;Integer&gt; limitStream = Stream.iterate(0, (x) -&gt; x + 3).limit(4); limitStream.forEach(System.out::println); System.out.println(&quot;---------------------------&quot;); // 通过generate方法创建 Stream&lt;Double&gt; mathStream = Stream.generate(Math::random).limit(3); mathStream.forEach(System.out::println); 输出结果： 1 2 3 --------------------------- 0 3 6 9 --------------------------- 0.6943196706753662 0.22025458942178056 0.6990157043068304 Stream和parallelSteam简单区分 stream是顺序流，由主线程按顺序对流执行操作，而parallelStream是并行流，内部以多线程并行执行的方式对流进行操作，但前提是流中的数据处理没有顺序要求。 例如筛选集合中的奇数，两者的处理不同之处： 如果流中的数据量足够大，并行流可以加快处速度。 除了直接创建并行流，还可以通过parallel()把顺序流转换成并行流： // 将顺序流转成并行流 Stream&lt;String&gt; parallel = stream.parallel(); Stream的使用在使用stream之前，先理解一个概念：Optional 。 Optional类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。更详细说明请见：https://www.runoob.com/java/java8-optional-class.html 接下来，大批代码向你袭来！我将用20个案例将Stream的使用整得明明白白，只要跟着敲一遍代码，就能很好地掌握。 对象准备： @Data @Builder @NoArgsConstructor @AllArgsConstructor class Staff &#123; /** * 姓名 */ private String name; /** * 薪资 */ private BigDecimal salary; /** * 年龄 */ private int age; /** * 性别 */ private String sex; &#125; 遍历/匹配（foreach/find/match） Stream也是支持类似集合的遍历和匹配元素的，只是Stream中的元素是以Optional类型存在的。Stream的遍历、匹配非常简单。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(7, 6, 9, 3, 8, 2, 1); // 遍历输出符合条件的元素 list.stream().filter(x -&gt; x &gt; 6).forEach(System.out::println); // 匹配第一个 Optional&lt;Integer&gt; findFirst = list.stream().filter(x -&gt; x &gt; 6).findFirst(); // 匹配任意（适用于并行流） Optional&lt;Integer&gt; findAny = list.parallelStream().filter(x -&gt; x &gt; 6).findAny(); // 是否包含符合特定条件的元素 boolean anyMatch = list.stream().anyMatch(x -&gt; x &lt; 6); Assert.isTrue(findAny.isPresent()); Assert.isTrue(findFirst.isPresent()); System.out.println(&quot;匹配第一个值：&quot; + findFirst.get()); System.out.println(&quot;匹配任意一个值：&quot; + findAny.get()); System.out.println(&quot;是否存在大于6的值：&quot; + anyMatch); &#125; &#125; 输出结果： 7 9 8 匹配第一个值：7 匹配任意一个值：8 是否存在大于6的值：true 筛选（filter） 筛选，是按照一定的规则校验流中的元素，将符合条件的元素提取到新的流中的操作。 案例一：筛选出Integer集合中大于7的元素，并打印出来 List&lt;Integer&gt; aList = Arrays.asList(6, 7, 3, 8, 1, 2, 9); List&lt;Integer&gt; res = aList.stream() .filter(x -&gt; x &gt; 7) .collect(Collectors.toList()); res.forEach(System.out::println); 结果： 8 9 案例二：筛选员工中工资高于8000的人，并形成新的集合。 形成新集合依赖collect（收集），后文有详细介绍。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900D, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000D, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900D, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200D, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500D, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800D, 30, &quot;女&quot;) ); List&lt;Staff&gt; staffs = asList.stream() .filter(staff -&gt; staff.getSalary() &gt; 8000) .collect(Collectors.toList()); staffs.forEach(staff -&gt; System.out.println(staff.toString())); &#125; &#125; 结果： Staff(name=Tom, salary=8900.0, age=29, sex=男) Staff(name=Anni, salary=8200.0, age=25, sex=男) Staff(name=Owen, salary=9500.0, age=27, sex=女) 聚合（max/min/count) max、min、count这些字眼你一定不陌生，没错，在mysql中我们常用它们进行数据统计。Java stream中也引入了这些概念和用法，极大地方便了我们对集合、数组的数据统计工作。 案例一：获取String集合中最长的元素。 List&lt;String&gt; list = Arrays.asList(&quot;adnm&quot;, &quot;admmt&quot;, &quot;pot&quot;, &quot;xbangd&quot;, &quot;weoujgsd&quot;); Optional&lt;String&gt; max = list.stream().max(Comparator.comparing(String::length)); System.out.println(&quot;最长的字符串：&quot; + max.get()); 输出： 最长的字符串：weoujgsd 案例二：获取Integer集合中的最大值 List&lt;Integer&gt; list = Arrays.asList(7, 6, 9, 4, 11, 6); // 自然排序 Optional&lt;Integer&gt; max = list.stream() .max(Integer::compareTo); // 自定义排序 Optional&lt;Integer&gt; max2 = list.stream() .max(new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o1.compareTo(o2) ; &#125; &#125;); System.out.println(&quot;自然排序结果：&quot; + max.get()); System.out.println(&quot;自定义排序结果：&quot; + max2.get()); 结果： 自然排序结果：11 自定义排序结果：11 案例三：获取员工工资最高的人。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900D, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000D, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900D, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200D, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500D, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800D, 30, &quot;女&quot;) ); Optional&lt;Staff&gt; staff = asList.stream() .max(Comparator.comparing(Staff::getSalary)); System.out.println(staff.get()); &#125; &#125; 输出结果： Staff(name=Owen, salary=9500.0, age=27, sex=女) 案例四：计算Integer集合中大于6的元素的个数。 List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 7, 9); long count = list.stream().filter(x -&gt; x &gt; 6).count(); System.out.println(&quot;集合中大于6的个数为：&quot; + count); 映射(map/flatMap) 映射，可以将一个流的元素按照一定的映射规则映射到另一个流中。分为map和flatMap： map：接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。 flatMap：接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。 案例一：英文字符串数组的元素全部改为大写。整数数组每个元素+3。 String[] strArr = &#123;&quot;abcd&quot;, &quot;bcdd&quot;, &quot;defde&quot;, &quot;fTr&quot;&#125;; List&lt;String&gt; upper = Arrays.stream(strArr) .map(String::toUpperCase) .collect(Collectors.toList()); System.out.println(&quot;元素大写：&quot; + upper); List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 7, 9); List&lt;Integer&gt; intList = list.stream() .map(x -&gt; x + 3) .collect(Collectors.toList()); System.out.println(&quot;每个元素加3：&quot; + intList); 结果： 元素大写：[ABCD, BCDD, DEFDE, FTR] 每个元素加3：[4, 5, 6, 7, 8, 10, 12] 案例二：将员工的薪资全部增加1000。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900D, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000D, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900D, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200D, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500D, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800D, 30, &quot;女&quot;) ); // 不改变原有集合 List&lt;Staff&gt; staffList = asList.stream() .map(staff -&gt; Staff.builder() .age(staff.getAge()) .name(staff.getName()) .salary(staff.getSalary() + 1000) .sex(staff.getSex()) .build()) .collect(Collectors.toList()); System.out.println(&quot;改变前：&quot; + asList); System.out.println(&quot;改变后：&quot; + staffList); System.out.println(&quot;-----------------------&quot;); List&lt;Staff&gt; staffList1 = asList.stream() .map(staff -&gt; &#123; staff.setSalary(staff.getSalary() + 1000); return staff; &#125;).collect(Collectors.toList()); System.out.println(&quot;改变前：&quot; + staffList); System.out.println(&quot;改变后：&quot; + staffList1); &#125; &#125; 结果： 改变前：[Staff(name=Tom, salary=8900.0, age=29, sex=男), Staff(name=Jack, salary=7000.0, age=20, sex=女), Staff(name=Lily, salary=7900.0, age=23, sex=女), Staff(name=Anni, salary=8200.0, age=25, sex=男), Staff(name=Owen, salary=9500.0, age=27, sex=女), Staff(name=Alisa, salary=7800.0, age=30, sex=女)] 改变后：[Staff(name=Tom, salary=9900.0, age=29, sex=男), Staff(name=Jack, salary=8000.0, age=20, sex=女), Staff(name=Lily, salary=8900.0, age=23, sex=女), Staff(name=Anni, salary=9200.0, age=25, sex=男), Staff(name=Owen, salary=10500.0, age=27, sex=女), Staff(name=Alisa, salary=8800.0, age=30, sex=女)] ----------------------- 改变前：[Staff(name=Tom, salary=9900.0, age=29, sex=男), Staff(name=Jack, salary=8000.0, age=20, sex=女), Staff(name=Lily, salary=8900.0, age=23, sex=女), Staff(name=Anni, salary=9200.0, age=25, sex=男), Staff(name=Owen, salary=10500.0, age=27, sex=女), Staff(name=Alisa, salary=8800.0, age=30, sex=女)] 改变后：[Staff(name=Tom, salary=9900.0, age=29, sex=男), Staff(name=Jack, salary=8000.0, age=20, sex=女), Staff(name=Lily, salary=8900.0, age=23, sex=女), Staff(name=Anni, salary=9200.0, age=25, sex=男), Staff(name=Owen, salary=10500.0, age=27, sex=女), Staff(name=Alisa, salary=8800.0, age=30, sex=女)] 案例三：将两个字符数组合并成一个新的字符数组。 List&lt;String&gt; list = Arrays.asList(&quot;m-k-l-a&quot;, &quot;1-3-5-7&quot;); List&lt;String&gt; strs = list.stream() .flatMap(str -&gt; &#123; // 将每个元素都转换为stream流 String[] split = str.split(&quot;-&quot;); return Arrays.stream(split); &#125;).collect(Collectors.toList()); System.out.println(&quot;合并前的数组：&quot; + list); System.out.println(&quot;合并后的集合：&quot; + strs); 结果： 合并前的数组：[m-k-l-a, 1-3-5-7] 合并后的集合：[m, k, l, a, 1, 3, 5, 7] 归约(reduce) 归约，也称缩减，顾名思义，是把一个流缩减成一个值，能实现对集合求和、求乘积和求最值操作。 案例一：求Integer集合的元素之和、乘积和最大值。 List&lt;Integer&gt; list = Arrays.asList(1, 3, 2, 8, 11, 4); // 求和方式一 Integer reduce1 = list.stream().reduce((x, y) -&gt; x + y).get(); // 求和方式二 Integer reduce2 = list.stream().reduce(Integer::sum).get(); // 求和方式三 Integer reduce3 = list.stream().reduce(0, Integer::sum); System.out.println(&quot;第一种求和结果：&quot; + reduce1); System.out.println(&quot;第二种求和结果：&quot; + reduce2); System.out.println(&quot;第三种求和结果：&quot; + reduce3); // 求乘积 Integer reduce4 = list.stream().reduce((x, y) -&gt; x * y).get(); // 求最大值方式一 Integer reduce5 = list.stream().reduce((x, y) -&gt; x &gt; y ? x : y).get(); // 求最大值方式二 Integer reduce6 = list.stream().reduce(0, Integer::max); System.out.println(&quot;求乘积：&quot; + reduce4); System.out.println(&quot;求最大值方式一：&quot; + reduce5); System.out.println(&quot;求最大值方式二：&quot; + reduce6); 结果： 第一种求和结果：29 第二种求和结果：29 第三种求和结果：29 求乘积：2112 求最大值方式一：11 求最大值方式二：11 案例二：求所有员工的工资之和和最高工资。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); // 最高工资方式一 Staff staff = asList.stream() .max(Comparator.comparing(Staff::getSalary)) .get(); // 最高工资方式二 Integer maxSalary = asList.stream().reduce(0, (max, p) -&gt; max &gt; p.getSalary() ? max : p.getSalary(), Integer::max); // 最高工资方式三 Integer maxSalary2 = asList.stream().reduce(0, (max, p) -&gt; max &gt; p.getSalary() ? max : p.getSalary(), (max1, max2) -&gt; max1 &gt; max2 ? max1 : max2); // 工资之和方式一 Integer sumSalary = asList.stream().map(Staff::getSalary).reduce(Integer::sum).get(); // 工资求和方式二 Integer sumSalary2 = asList.stream().reduce(0, (sum, p) -&gt; sum += p.getSalary(), (sum1, sum2) -&gt; sum1 + sum2); // 求工资之和方式三 Integer sumSalary3 = asList.stream().reduce(0, (sum, p) -&gt; sum += p.getSalary(), Integer::sum); System.out.println(&quot;最高工资一：&quot; + staff.getSalary()); System.out.println(&quot;最高工资二：&quot; + maxSalary); System.out.println(&quot;最高工资三：&quot; + maxSalary2); System.out.println(&quot;工资之和方式一：&quot; + sumSalary); System.out.println(&quot;工资之和方式二：&quot; + sumSalary2); System.out.println(&quot;工资之和方式三：&quot; + sumSalary3); &#125; &#125; 结构： 最高工资一：9500 最高工资二：9500 最高工资三：9500 工资之和方式一：49300 工资之和方式二：49300 工资之和方式三：49300 收集(collect)collect，收集，可以说是内容最繁多、功能最丰富的部分了。从字面上去理解，就是把一个流收集起来，最终可以是收集成一个值也可以收集成一个新的集合。 collect主要依赖java.util.stream.Collectors类内置的静态方法。 归集(toList/toSet/toMap)因为流不存储数据，那么在流中的数据完成处理后，需要将流中的数据重新归集到新的集合里。toList、toSet和toMap比较常用，另外还有toCollection、toConcurrentMap等复杂一些的用法。 下面用一个案例演示toList、toSet和toMap： public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); List&lt;Integer&gt; list = Arrays.asList(1, 6, 3, 4, 6, 7, 9, 6, 20); List&lt;Integer&gt; listNew = list.stream().filter(x -&gt; x % 2 == 0).collect(Collectors.toList()); Set&lt;Integer&gt; set = list.stream().filter(x -&gt; x % 2 == 0).collect(Collectors.toSet()); Map&lt;?, Staff&gt; map = asList.stream().filter(p -&gt; p.getSalary() &gt; 9000) .collect(Collectors.toMap(Staff::getName, p -&gt; p)); System.out.println(&quot;toList:&quot; + listNew); System.out.println(&quot;toSet:&quot; + set); System.out.println(&quot;toMap:&quot; + map); &#125; &#125; 结果： toList:[6, 4, 6, 6, 20] toSet:[4, 20, 6] toMap:&#123;Owen=Staff(name=Owen, salary=9500, age=27, sex=女)&#125; 统计(count/averaging)Collectors提供了一系列用于数据统计的静态方法： 计数：count 平均值：averagingInt、averagingLong、averagingDouble 最值：maxBy、minBy 求和：summingInt、summingLong、summingDouble 统计以上所有：summarizingInt、summarizingLong、summarizingDouble 案例：统计员工人数、平均工资、工资总额、最高工资。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Anni&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); // 求总数 Long count = asList.stream().collect(Collectors.counting()); // 求平均工资 Double average = asList.stream().collect(Collectors.averagingDouble(Staff::getSalary)); // 求最高工资 Optional&lt;Integer&gt; max = asList.stream().map(Staff::getSalary).collect(Collectors.maxBy(Integer::compare)); // 求工资之和 Integer sum = asList.stream().collect(Collectors.summingInt(Staff::getSalary)); // 一次性统计所有信息 DoubleSummaryStatistics collect = asList.stream().collect(Collectors.summarizingDouble(Staff::getSalary)); System.out.println(&quot;员工总数：&quot; + count); System.out.println(&quot;员工平均工资：&quot; + average); System.out.println(&quot;员工工资总和：&quot; + sum); System.out.println(&quot;员工工资所有统计：&quot; + collect); &#125; &#125; 结果： 员工总数：6 员工平均工资：8216.666666666666 员工工资总和：49300 员工工资所有统计：DoubleSummaryStatistics&#123;count=6, sum=49300.000000, min=7000.000000, average=8216.666667, max=9500.000000&#125; 分组(partitioningBy/groupingBy) 分区：将stream按条件分为两个Map，比如员工按薪资是否高于8000分为两部分。 分组：将集合分为多个Map，比如员工按性别分组。有单级分组和多级分组。 案例：将员工按薪资是否高于8000分为两部分；将员工按性别和地区分组 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); Map&lt;Boolean, List&lt;Staff&gt;&gt; part = asList.stream() .collect(Collectors.partitioningBy(x -&gt; x.getSalary() &gt; 8000)); // 将员工按性别分组 Map&lt;String, List&lt;Staff&gt;&gt; group = asList.stream() .collect(Collectors.groupingBy(Staff::getSex)); // 将员工先按性别分组，再按地区分组 Map&lt;String, Map&lt;String, List&lt;Staff&gt;&gt;&gt; group2 = asList.stream() .collect(Collectors.groupingBy(Staff::getSex, Collectors.groupingBy(Staff::getName))); System.out.println(&quot;员工按薪资是否大于8000分组情况：&quot; + part); System.out.println(&quot;员工按性别分组情况：&quot; + group); System.out.println(&quot;员工按性别、姓名：&quot; + group2); &#125; &#125; 结果： 员工按薪资是否大于8000分组情况： &#123;false=[Staff(name=Jack, salary=7000, age=20, sex=女), Staff(name=Lily, salary=7900, age=23, sex=女), Staff(name=Alisa, salary=7800, age=30, sex=女)], true=[Staff(name=Tom, salary=8900, age=29, sex=男), Staff(name=Alisa, salary=8200, age=25, sex=男), Staff(name=Owen, salary=9500, age=27, sex=女)]&#125; 员工按性别分组情况： &#123;女=[Staff(name=Jack, salary=7000, age=20, sex=女), Staff(name=Lily, salary=7900, age=23, sex=女), Staff(name=Owen, salary=9500, age=27, sex=女), Staff(name=Alisa, salary=7800, age=30, sex=女)], 男=[Staff(name=Tom, salary=8900, age=29, sex=男), Staff(name=Alisa, salary=8200, age=25, sex=男)]&#125; 员工按性别、姓名： &#123;女=&#123;Owen=[Staff(name=Owen, salary=9500, age=27, sex=女)], Alisa=[Staff(name=Alisa, salary=7800, age=30, sex=女)], Jack=[Staff(name=Jack, salary=7000, age=20, sex=女)], Lily=[Staff(name=Lily, salary=7900, age=23, sex=女)]&#125;, 男=&#123;Tom=[Staff(name=Tom, salary=8900, age=29, sex=男)], Alisa=[Staff(name=Alisa, salary=8200, age=25, sex=男)]&#125;&#125; 接合(joining)joining可以将stream中的元素用特定的连接符（没有的话，则直接连接）连接成一个字符串。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); String names = asList.stream().map(Staff::getName).collect(Collectors.joining(&quot;,&quot;)); System.out.println(&quot;所有员工的姓名：&quot; + names); List&lt;String&gt; list = Arrays.asList(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;); String string = list.stream().collect(Collectors.joining(&quot;-&gt;&quot;)); System.out.println(&quot;拼接后的字符串：&quot; + string); &#125; &#125; 结果： 所有员工的姓名：Tom,Jack,Lily,Alisa,Owen,Alisa 拼接后的字符串：A-&gt;B-&gt;C 归约(reducing)Collectors类提供的reducing方法，相比于stream本身的reduce方法，增加了对自定义归约的支持。 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); // 每个员工减去起征点后的薪资之和 Integer sum = asList.stream().collect(Collectors.reducing(0, Staff::getSalary, (i, j) -&gt; (i + j - 5000))); // Integer sum = asList.stream().map(Staff::getSalary).reduce(0, (i, j) -&gt; (i + j - 5000)); System.out.println(&quot;员工扣税薪资总和：&quot; + sum); // stream的reduce Optional&lt;Integer&gt; sum2 = asList.stream().map(Staff::getSalary).reduce(Integer::sum); System.out.println(&quot;员工薪资总和：&quot; + sum2.get()); &#125; &#125; 结果： 员工扣税薪资总和：19300 员工薪资总和：49300 排序(sorted)sorted，中间操作。有两种排序： sorted()：自然排序，流中元素需实现Comparable接口 sorted(Comparator com)：Comparator排序器自定义排序 案例：将员工按工资由高到低（工资一样则按年龄由大到小）排序 public class Main &#123; public static void main(String[] args) &#123; List&lt;Staff&gt; asList = Arrays.asList( new Staff(&quot;Tom&quot;, 8900, 29, &quot;男&quot;), new Staff(&quot;Jack&quot;, 7000, 20, &quot;女&quot;), new Staff(&quot;Lily&quot;, 7900, 23, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 8200, 25, &quot;男&quot;), new Staff(&quot;Owen&quot;, 9500, 27, &quot;女&quot;), new Staff(&quot;Alisa&quot;, 7800, 30, &quot;女&quot;) ); // 按工资升序排序（自然排序） List&lt;String&gt; newList = asList.stream().sorted(Comparator.comparing(Staff::getSalary)).map(Staff::getName) .collect(Collectors.toList()); // 按工资倒序排序 List&lt;String&gt; newList2 = asList.stream().sorted(Comparator.comparing(Staff::getSalary).reversed()) .map(Staff::getName).collect(Collectors.toList()); // 先按工资再按年龄升序排序 List&lt;String&gt; newList3 = asList.stream() .sorted(Comparator.comparing(Staff::getSalary).thenComparing(Staff::getAge)).map(Staff::getName) .collect(Collectors.toList()); // 先按工资再按年龄自定义排序（降序） List&lt;String&gt; newList4 = asList.stream().sorted((p1, p2) -&gt; &#123; if (p1.getSalary() == p2.getSalary()) &#123; return p2.getAge() - p1.getAge(); &#125; else &#123; return p2.getSalary() - p1.getSalary(); &#125; &#125;).map(Staff::getName).collect(Collectors.toList()); System.out.println(&quot;按工资升序排序：&quot; + newList); System.out.println(&quot;按工资降序排序：&quot; + newList2); System.out.println(&quot;先按工资再按年龄升序排序：&quot; + newList3); System.out.println(&quot;先按工资再按年龄自定义降序排序：&quot; + newList4); &#125; &#125; 结果： 按工资升序排序：[Jack, Alisa, Lily, Alisa, Tom, Owen] 按工资降序排序：[Owen, Tom, Alisa, Lily, Alisa, Jack] 先按工资再按年龄升序排序：[Jack, Alisa, Lily, Alisa, Tom, Owen] 先按工资再按年龄自定义降序排序：[Owen, Tom, Alisa, Lily, Alisa, Jack] 提取/组合流也可以进行合并、去重、限制、跳过等操作。 去重： 跳过： 限制： public class Main &#123; public static void main(String[] args) &#123; String[] arr1 = &#123; &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot; &#125;; String[] arr2 = &#123; &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot; &#125;; Stream&lt;String&gt; stream1 = Stream.of(arr1); Stream&lt;String&gt; stream2 = Stream.of(arr2); // concat:合并两个流 distinct：去重 List&lt;String&gt; newList = Stream.concat(stream1, stream2).distinct().collect(Collectors.toList()); // limit：限制从流中获得前n个数据 List&lt;Integer&gt; collect = Stream.iterate(1, x -&gt; x + 2).limit(10).collect(Collectors.toList()); // skip：跳过前n个数据 List&lt;Integer&gt; collect2 = Stream.iterate(1, x -&gt; x + 2).skip(1).limit(5).collect(Collectors.toList()); System.out.println(&quot;流合并：&quot; + newList); System.out.println(&quot;limit：&quot; + collect); System.out.println(&quot;skip：&quot; + collect2); &#125; &#125; 结果： 流合并：[a, b, c, d, e, f, g] limit：[1, 3, 5, 7, 9, 11, 13, 15, 17, 19] skip：[3, 5, 7, 9, 11]","categories":[{"name":"01java","slug":"01java","permalink":"https://zabernism.github.io/blog/categories/01java/"},{"name":"09Stream","slug":"01java/09Stream","permalink":"https://zabernism.github.io/blog/categories/01java/09Stream/"}],"tags":[{"name":"java","slug":"java","permalink":"https://zabernism.github.io/blog/tags/java/"}]},{"title":"TCP的11种状态","slug":"08 计算机网络/TCP的11种状态","date":"2021-08-03T01:39:50.793Z","updated":"2021-08-03T05:05:30.811Z","comments":true,"path":"2021/08/03/08 计算机网络/TCP的11种状态/","link":"","permalink":"https://zabernism.github.io/blog/2021/08/03/08%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E7%9A%8411%E7%A7%8D%E7%8A%B6%E6%80%81/","excerpt":"","text":"TCP的11种状态TCP三次握手建立连接Tcp头部 六个标志位中，我们要用到三个： SYN：SYN= 1 表示这是一个连接请求或连接接受报文。在建立连接时用来进行同步序号（个人理解是，在建立连接的时候，提醒对方记录本方的起始序号）。当SYN=1而ACK=0时，表明这是一个连接请求报文段。对方若是同意建立连接，则应响应的报文段中使SYN=1、ACK=1。因此SYN=1表示该报文是一个连接请求报文或者是一个连接请求接收报文。 ACK：确认号只有在该位设置为1的时候才生效，当该位为0是表示确认号无效。TCP规定，在TCP连接建立后所有传送的数据报文段ACK都必须设置为1。 FIN：当 FIN = 1 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。 此外我们还需要用到序号和确认号： 序号：占4个字节，它的范围在0-2^32-1，序号随着通信的进行不断的递增，当达到最大值的时候重新回到0在开始递增。TCP是面向字节流的，在一个TCP连接中传送的字节流中的每一个字节都按照顺序编号。整个要传送的字节流的起始号必须在连接建立时设置。首部中的序列号字段指的是本报文段所发送的数据的第一个字节的序号。例如，一个报文序号是301，而携带的数据共有100字节。则表示本次报文中的序号是301，下一个报文的序号是401.重复一下，每一个报文的序号是该报文包含的字节中第一个字节的编号。 确认号：占4个字节，确认号，是对下一个想要接受的字节的期望，这里隐式确认了对上一个数据包的成功接收。如上例，在成功接收了序号为301的数据包，想要接收下一个数据包因为上个数据包包含100字节，所以此时的确认号应该是401，表示希望接收下一个序号是401的数据包。 三次握手过程： 过程描述： 首先由Client发出请求连接即 SYN=1 ACK=0 (请看头字段的介绍)，TCP规定SYN=1时不能携带数据，但要消耗一个序号,因此声明自己的序号是 seq=x。 然后 Server 进行回复确认，即 SYN=1 ACK=1 seq=y，ack=x+1。 再然后 Client 再进行一次确认，但不用SYN 了，这时即为 ACK=1, seq=x+1，ack=y+1。 为什么要进行三次握手（两次确认）： 为什么A还要发送一侧确认呢？这主要是为了防止已失效的连接请求报文突然又传送到了B，因而产生错误。 所谓“已失效的连接请求报文段”是这样产生的。考虑一种正常情况。A发出连接请求，但因连接请求丢失而未收到确认。于是A再次重传一次连接请求。后来收到了确认建立了连接。数据传输完毕后，就释放了连接。A供发送了两个连接请求的报文段，其中第一个丢失，第二个到达了B。没有“已失效的连接请求报文段”。 现假定出现一种异常情况，即A发出的第一个连接请求报文段并没有丢失，而是在某些网络节点长时间滞留了，以致延误到连接释放以后的某个时间才到B。本来这是一个已失效的报文段。但是B收到此失效的连接请求报文段后，就误认为是A有发出一次新的连接请求。于是就向A发出确认报文段，同意建立连接。假定不采用三次握手，那么只要B发出确认，新的连接就建立了。 由于现在A并没有发出建立连接的请求，因此不会理睬B的确认，也不会向B发送数据。但B却以为新的运输连接已经建立了，并一直等待A发来数据。B的许多资源就这样拜拜浪费了。 采用三次握手的办法可以防止上述现象的发生。例如在刚才的情况下，A不会向B的确认发出确认。B由于收不到确认，就知道A并没有要求建立连接。 另一种解释： 这个问题的本质是, 信道不可靠, 但是通信双发需要就某个问题达成一致. 而要解决这个问题, 无论你在消息中包含什么信息, 三次通信是理论上的最小值. 所以三次握手不是TCP本身的要求, 而是为了满足”在不可靠信道上可靠地传输信息”这一需求所导致的. 请注意这里的本质需求,信道不可靠, 数据传输要可靠. 三次达到了, 那后面你想接着握手也好, 发数据也好, 跟进行可靠信息传输的需求就没关系了. 因此,如果信道是可靠的, 即无论什么时候发出消息, 对方一定能收到, 或者你不关心是否要保证对方收到你的消息, 那就能像UDP那样直接发送消息就可以了”。这可视为对“三次握手”目的的另一种解答思路。 四次挥手关闭连接 当客户A 没有东西要发送时就要释放 A 这边的连接，A会发送一个报文（没有数据），其中 FIN 设置为1, 服务器B收到后会给应用程序一个信，这时A那边的连接已经关闭，即A不再发送信息（但仍可接收信息）。 A收到B的确认后进入等待状态，等待B请求释放连接， B数据发送完成后就向A请求连接释放，也是用FIN=1 表示， 并且用 ack = u+1(如图）， A收到后回复一个确认信息，并进入 TIME_WAIT 状态， 等待 2MSL 时间。 l 为什么要等待呢？ l 为了防止这种情况：A接到B的释放连接请求后会发送一个确认信息，但是如果这个确认信息丢了，也就是B没有收到确认释放连接，那么B就会重发一个释放连接请求，这时候A还处于TIME_WAIT状态，所以会再次发送一个确认信息。 l Q2为什么TIME_WAIT 状态还需要等2*MSL秒之后才能返回到CLOSED 状态呢？ l A2因为虽然双方都同意关闭连接了，而且握手的4个报文也都发送完毕，按理可以直接回到CLOSED 状态（就好比从SYN_SENT 状态到ESTABLISH 状态那样），但是我们必须假想网络是不可靠的，你无法保证你最后发送的ACK报文一定会被对方收到，就是说对方处于LAST_ACK 状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT 状态的作用就是用来重发可能丢失的ACK报文。 11种状态 简单解释： l CLOSED：初始状态，表示TCP连接是“关闭着的”或“未打开的”。 l LISTEN ：表示服务器端的某个SOCKET处于监听状态，可以接受客户端的连接。 l SYN_RCVD ：表示服务器接收到了来自客户端请求连接的SYN报文。在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用netstat很难看到这种状态，除非故意写一个监测程序，将三次TCP握手过程中最后一个ACK报文不予发送。当TCP连接处于此状态时，再收到客户端的ACK报文，它就会进入到ESTABLISHED 状态。 l SYN_SENT ：这个状态与SYN_RCVD 状态相呼应，当客户端SOCKET执行connect()进行连接时，它首先发送SYN报文，然后随即进入到SYN_SENT 状态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT 状态表示客户端已发送SYN报文。 l ESTABLISHED ：表示TCP连接已经成功建立。 l FIN_WAIT_1 ：这个状态得好好解释一下，其实FIN_WAIT_1 和FIN_WAIT_2 两种状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET进入到FIN_WAIT_1 状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2 状态。当然在实际的正常情况下，无论对方处于任何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1 状态一般是比较难见到的，而FIN_WAIT_2 状态有时仍可以用netstat看到。 l FIN_WAIT_2 ：上面已经解释了这种状态的由来，实际上FIN_WAIT_2状态下的SOCKET表示半连接，即有一方调用close()主动要求关闭连接。注意：FIN_WAIT_2 是没有超时的（不像TIME_WAIT 状态），这种状态下如果对方不关闭（不配合完成4次挥手过程），那这个 FIN_WAIT_2 状态将一直保持到系统重启，越来越多的FIN_WAIT_2 状态会导致内核crash。 l TIME_WAIT ：表示收到了对方的FIN报文，并发送出了ACK报文。 TIME_WAIT状态下的TCP连接会等待2*MSL（Max Segment Lifetime，最大分段生存期，指一个TCP报文在Internet上的最长生存时间。每个具体的TCP协议实现都必须选择一个确定的MSL值，RFC 1122建议是2分钟，但BSD传统实现采用了30秒，Linux可以cat /proc/sys/net/ipv4/tcp_fin_timeout看到本机的这个值），然后即可回到CLOSED 可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（这种情况应该就是四次挥手变成三次挥手的那种情况） l CLOSING ：这种状态在实际情况中应该很少见，属于一种比较罕见的例外状态。正常情况下，当一方发送FIN报文后，按理来说是应该先收到（或同时收到）对方的ACK报文，再收到对方的FIN报文。但是CLOSING 状态表示一方发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什么情况下会出现此种情况呢？那就是当双方几乎在同时close()一个SOCKET的话，就出现了双方同时发送FIN报文的情况，这是就会出现CLOSING 状态，表示双方都正在关闭SOCKET连接。 l CLOSE_WAIT ：表示正在等待关闭。怎么理解呢？当对方close()一个SOCKET后发送FIN报文给自己，你的系统毫无疑问地将会回应一个ACK报文给对方，此时TCP连接则进入到CLOSE_WAIT状态。接下来呢，你需要检查自己是否还有数据要发送给对方，如果没有的话，那你也就可以close()这个SOCKET并发送FIN报文给对方，即关闭自己到对方这个方向的连接。有数据的话则看程序的策略，继续发送或丢弃。简单地说，当你处于CLOSE_WAIT 状态下，需要完成的事情是等待你去关闭连接。 l LAST_ACK ：当被动关闭的一方在发送FIN报文后，等待对方的ACK报文的时候，就处于LAST_ACK 状态。当收到对方的ACK报文后，也就可以进入到CLOSED 可用状态了。 CLOSING状态： ![image-20210803130321318](/Users/zhangshuai/Library/Application Support/typora-user-images/image-20210803130321318.png)","categories":[{"name":"08计算机网络","slug":"08计算机网络","permalink":"https://zabernism.github.io/blog/categories/08%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://zabernism.github.io/blog/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"hexo使用主题之后分类不显示","slug":"随笔/hexo分类不显示问题解决","date":"2021-07-31T10:08:40.042Z","updated":"2021-08-03T13:31:36.929Z","comments":true,"path":"2021/07/31/随笔/hexo分类不显示问题解决/","link":"","permalink":"https://zabernism.github.io/blog/2021/07/31/%E9%9A%8F%E7%AC%94/hexo%E5%88%86%E7%B1%BB%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","excerpt":"","text":"hexo分类不显示问题检查站点文件的_config.yml# Directory source_dir: source public_dir: public tag_dir: tags archive_dir: archives category_dir: categories 观看tag_dir 等文件设置是否正确，只有设置正确，才能被hexo加载 创建新的页面创建新的页面可以在博客的根目录使用命令：hexo new page &#39;&#39; 此时根目录的source文件夹下会相应创建一个文件夹，并且其中包含一个index.md文件（这个过程可以手动创建）；例如，我创建一个tags页面，那么source文件夹下就会出现一个tags文件夹，在tags文件夹下应该有一个index.md 最关键的一步使用命令创建文件夹的话，index.md是没有指定类型的应当是 --- title: 标签 date: 2021-07-31 15:28:39 --- 此时需要手动添加tpye与layout 需要注意的是：type对应的是站点配置文件中的名字tags，layout则是对应的主题的layout文件夹下的ejs类型的文件。例如我的主题文件夹如下： 明显可以看出我的主题的layout文件下是没有tags.ejs文件的，换成了tag.ejs文件，所以我的主题配置文件如下 --- type: tags layout: tag ---","categories":[{"name":"随笔","slug":"随笔","permalink":"https://zabernism.github.io/blog/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://zabernism.github.io/blog/tags/%E9%9A%8F%E7%AC%94/"},{"name":"hexo","slug":"hexo","permalink":"https://zabernism.github.io/blog/tags/hexo/"}]},{"title":"typora设置自动编号","slug":"随笔/typora设置标题自动编号","date":"2021-07-27T09:52:15.561Z","updated":"2021-08-03T13:31:38.792Z","comments":true,"path":"2021/07/27/随笔/typora设置标题自动编号/","link":"","permalink":"https://zabernism.github.io/blog/2021/07/27/%E9%9A%8F%E7%AC%94/typora%E8%AE%BE%E7%BD%AE%E6%A0%87%E9%A2%98%E8%87%AA%E5%8A%A8%E7%BC%96%E5%8F%B7/","excerpt":"","text":"Typora设置自动编号找到主题文件文件-&gt;偏好设置-&gt;外观 新建base.user.css文件 添加代码/************************************** * Header Counters in TOC **************************************/ /* No link underlines in TOC */ .md-toc-inner &#123; text-decoration: none; &#125; .md-toc-content &#123; counter-reset: h1toc &#125; .md-toc-h1 &#123; margin-left: 0; font-size: 1.5rem; counter-reset: h2toc &#125; .md-toc-h2 &#123; font-size: 1.1rem; margin-left: 2rem; counter-reset: h3toc &#125; .md-toc-h3 &#123; margin-left: 3rem; font-size: .9rem; counter-reset: h4toc &#125; .md-toc-h4 &#123; margin-left: 4rem; font-size: .85rem; counter-reset: h5toc &#125; .md-toc-h5 &#123; margin-left: 5rem; font-size: .8rem; counter-reset: h6toc &#125; .md-toc-h6 &#123; margin-left: 6rem; font-size: .75rem; &#125; .md-toc-h1:before &#123; color: black; counter-increment: h1toc; content: counter(h1toc) &quot;. &quot; &#125; .md-toc-h1 .md-toc-inner &#123; margin-left: 0; &#125; .md-toc-h2:before &#123; color: black; counter-increment: h2toc; content: counter(h1toc) &quot;. &quot; counter(h2toc) &quot;. &quot; &#125; .md-toc-h2 .md-toc-inner &#123; margin-left: 0; &#125; .md-toc-h3:before &#123; color: black; counter-increment: h3toc; content: counter(h1toc) &quot;. &quot; counter(h2toc) &quot;. &quot; counter(h3toc) &quot;. &quot; &#125; .md-toc-h3 .md-toc-inner &#123; margin-left: 0; &#125; .md-toc-h4:before &#123; color: black; counter-increment: h4toc; content: counter(h1toc) &quot;. &quot; counter(h2toc) &quot;. &quot; counter(h3toc) &quot;. &quot; counter(h4toc) &quot;. &quot; &#125; .md-toc-h4 .md-toc-inner &#123; margin-left: 0; &#125; .md-toc-h5:before &#123; color: black; counter-increment: h5toc; content: counter(h1toc) &quot;. &quot; counter(h2toc) &quot;. &quot; counter(h3toc) &quot;. &quot; counter(h4toc) &quot;. &quot; counter(h5toc) &quot;. &quot; &#125; .md-toc-h5 .md-toc-inner &#123; margin-left: 0; &#125; .md-toc-h6:before &#123; color: black; counter-increment: h6toc; content: counter(h1toc) &quot;. &quot; counter(h2toc) &quot;. &quot; counter(h3toc) &quot;. &quot; counter(h4toc) &quot;. &quot; counter(h5toc) &quot;. &quot; counter(h6toc) &quot;. &quot; &#125; .md-toc-h6 .md-toc-inner &#123; margin-left: 0; &#125; /************************************** * Header Counters in Content **************************************/ /** initialize css counter */ #write &#123; counter-reset: h1 &#125; h1 &#123; counter-reset: h2 &#125; h2 &#123; counter-reset: h3 &#125; h3 &#123; counter-reset: h4 &#125; h4 &#123; counter-reset: h5 &#125; h5 &#123; counter-reset: h6 &#125; /** put counter result into headings */ #write h1:before &#123; counter-increment: h1; content: counter(h1) &quot;. &quot; &#125; #write h2:before &#123; counter-increment: h2; content: counter(h1) &quot;.&quot; counter(h2) &quot;. &quot; &#125; #write h3:before, h3.md-focus.md-heading:before &#123; /*override the default style for focused headings */ counter-increment: h3; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;. &quot; &#125; #write h4:before, h4.md-focus.md-heading:before &#123; counter-increment: h4; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;. &quot; &#125; #write h5:before, h5.md-focus.md-heading:before &#123; counter-increment: h5; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;.&quot; counter(h5) &quot;. &quot; &#125; #write h6:before, h6.md-focus.md-heading:before &#123; counter-increment: h6; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;.&quot; counter(h5) &quot;.&quot; counter(h6) &quot;. &quot; &#125; /** override the default style for focused headings */ #write&gt;h3.md-focus:before, #write&gt;h4.md-focus:before, #write&gt;h5.md-focus:before, #write&gt;h6.md-focus:before, h3.md-focus:before, h4.md-focus:before, h5.md-focus:before, h6.md-focus:before &#123; color: inherit; border: inherit; border-radius: inherit; position: inherit; left: initial; float: none; top: initial; font-size: inherit; padding-left: inherit; padding-right: inherit; vertical-align: inherit; font-weight: inherit; line-height: inherit; &#125; /************************************** * Header Counters in sidebar **************************************/ .sidebar-content &#123; counter-reset: h1 &#125; .outline-h1 &#123; counter-reset: h2 &#125; .outline-h2 &#123; counter-reset: h3 &#125; .outline-h3 &#123; counter-reset: h4 &#125; .outline-h4 &#123; counter-reset: h5 &#125; .outline-h5 &#123; counter-reset: h6 &#125; .outline-h1&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h1; content: counter(h1) &quot;. &quot; &#125; .outline-h2&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h2; content: counter(h1) &quot;.&quot; counter(h2) &quot;. &quot; &#125; .outline-h3&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h3; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;. &quot; &#125; .outline-h4&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h4; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;. &quot; &#125; .outline-h5&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h5; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;.&quot; counter(h5) &quot;. &quot; &#125; .outline-h6&gt;.outline-item&gt;.outline-label:before &#123; counter-increment: h6; content: counter(h1) &quot;.&quot; counter(h2) &quot;.&quot; counter(h3) &quot;.&quot; counter(h4) &quot;.&quot; counter(h5) &quot;.&quot; counter(h6) &quot;. &quot; &#125; 重启参考文章：https://blog.csdn.net/zhongqi2513/article/details/105123345/?utm_medium=distribute.pc_relevant.none-task-blog-title-6","categories":[{"name":"随笔","slug":"随笔","permalink":"https://zabernism.github.io/blog/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://zabernism.github.io/blog/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"Spring注解","slug":"05 Spring/其他/Spring注解","date":"2021-07-27T09:52:15.561Z","updated":"2021-07-31T09:03:00.506Z","comments":true,"path":"2021/07/27/05 Spring/其他/Spring注解/","link":"","permalink":"https://zabernism.github.io/blog/2021/07/27/05%20Spring/%E5%85%B6%E4%BB%96/Spring%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"注解@JsonProperty @JsonProperty 此注解用于属性上，作用是把该属性的名称序列化为另外一个名称，如把trueName属性序列化为name @JsonProperty(&quot;name&quot;) private String trueName; @FeignClient注解FeignClient注解被@Target(ElementType.TYPE)修饰，表示FeignClient注解的作用目标在接口上 @FeignClient(name = ``&quot;github-client&quot;``, url = ``&quot;https://api.github.com&quot;``, configuration = GitHubExampleConfig.``class``) public` `interface` `GitHubClient &#123; ``@RequestMapping(value = ``&quot;/search/repositories&quot;``, method = RequestMethod.GET) ``String searchRepo(@RequestParam(``&quot;q&quot;``) String queryStr); &#125; 声明接口之后，在代码中通过@Resource注入之后即可使用。@FeignClient标签的常用属性如下： name：指定FeignClient的名称，如果项目使用了Ribbon，name属性会作为微服务的名称，用于服务发现 url: url一般用于调试，可以手动指定@FeignClient调用的地址 decode404:当发生http 404错误时，如果该字段位true，会调用decoder进行解码，否则抛出FeignException configuration: Feign配置类，可以自定义Feign的Encoder、Decoder、LogLevel、Contract fallback: 定义容错的处理类，当调用远程接口失败或超时时，会调用对应接口的容错逻辑，fallback指定的类必须实现@FeignClient标记的接口 fallbackFactory: 工厂类，用于生成fallback类示例，通过这个属性我们可以实现每个接口通用的容错逻辑，减少重复的代码 path: 定义当前FeignClient的统一前缀 @Valid注解 用于验证注解是否符合要求，直接加在变量user之前，在变量中添加验证信息的要求，当不符合要求时就会在方法中返回message 的错误提示信息。 @RestController @RequestMapping(&quot;/user&quot;) public class UserController &#123; @PostMapping public User create (@Valid @RequestBody User user) &#123; System.out.println(user.getId()); System.out.println(user.getUsername()); System.out.println(user.getPassword()); user.setId(&quot;1&quot;); return user; &#125; &#125; 然后在 User 类中添加验证信息的要求 public class User &#123; private String id; @NotBlank(message = &quot;密码不能为空&quot;) private String password; &#125; @NotBlank 注解所指的 password 字段，表示验证密码不能为空，如果为空的话，上面 Controller 中的 create 方法会将message 中的”密码不能为空”返回。 更多信息要求 @Null 限制只能为null @NotNull 限制必须不为null @AssertFalse 限制必须为false @AssertTrue 限制必须为true @DecimalMax(value) 限制必须为一个不大于指定值的数字 @DecimalMin(value) 限制必须为一个不小于指定值的数字 @Digits(integer,fraction) 限制必须为一个小数，且整数部分的位数不能超过integer，小数部分的位数不能超过fraction @Future 限制必须是一个将来的日期 @Max(value) 限制必须为一个不大于指定值的数字 @Min(value) 限制必须为一个不小于指定值的数字 @Past 限制必须是一个过去的日期 @Pattern(value) 限制必须符合指定的正则表达式 @Size(max,min) 限制字符长度必须在min到max之间 @Past 验证注解的元素值（日期类型）比当前时间早 @NotEmpty 验证注解的元素值不为null且不为空（字符串长度不为0、集合大小不为0） @NotBlank 验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于@NotEmpty，@NotBlank只应用于字符串且在比较时会去除字符串的空格 @Email 验证注解的元素值是Email，也可以通过正则表达式和flag指定自定义的email格式","categories":[{"name":"05Spring","slug":"05Spring","permalink":"https://zabernism.github.io/blog/categories/05Spring/"},{"name":"其他","slug":"05Spring/其他","permalink":"https://zabernism.github.io/blog/categories/05Spring/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://zabernism.github.io/blog/tags/spring/"},{"name":"注解","slug":"注解","permalink":"https://zabernism.github.io/blog/tags/%E6%B3%A8%E8%A7%A3/"}]},{"title":"JAVA面试题","slug":"面试题整理/01 JAVA","date":"2021-07-27T09:52:15.561Z","updated":"2021-08-07T06:48:36.559Z","comments":true,"path":"2021/07/27/面试题整理/01 JAVA/","link":"","permalink":"https://zabernism.github.io/blog/2021/07/27/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%95%B4%E7%90%86/01%20JAVA/","excerpt":"","text":"一、基础篇网络基础TCP三次握手​ 三次握手过程： ​ 客户端——发送带有SYN标志的数据包——服务端 一次握手 Client进入syn_sent状态 ​ 服务端——发送带有SYN/ACK标志的数据包——客户端 二次握手 服务端进入syn_rcvd ​ 客户端——发送带有ACK标志的数据包——服务端 三次握手 连接就进入Established状态 ​ 解释： SYN：同步序列编号（Synchronize Sequence Numbers）。是TCP/IP建立连接时使用的握手信号 ACK：标志位 SYN_SENT：表示请求连接。传输控制协议（英语：Transmission Control Protocol, TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义 ​ 为什么三次： ​ 主要是为了建立可靠的通信信道，保证客户端与服务端同时具备发送、接收数据的能力 ​ 为什么两次不行？ ​ 1、防止已失效的请求报文又传送到了服务端，建立了多余的链接，浪费资源 ​ 2、 两次握手只能保证单向连接是畅通的。（为了实现可靠数据传输， TCP 协议的通信双方， 都必须维 护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 三次握手的过程即是通信双方 相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤；如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认） **TCP四次挥手过程 ** 四次挥手过程： ​ 客户端——发送带有FIN标志的数据包——服务端，关闭与服务端的连接 ，客户端进入FIN-WAIT-1状态 ​ 服务端收到这个 FIN，它发回⼀ 个 ACK，确认序号为收到的序号加1，服务端就进入了CLOSE-WAIT状态 ​ 服务端——发送⼀个FIN数据包——客户端，关闭与客户端的连接，客户端就进入FIN-WAIT-2状态 ​ 客户端收到这个 FIN，发回 ACK 报⽂确认，并将确认序号设置为收到序号加1，TIME-WAIT状态 为什么四次： ​ 因为需要确保客户端与服务端的数据能够完成传输。 CLOSE-WAIT： ​ 这种状态的含义其实是表示在等待关闭 TIME-WAIT： ​ 为了解决网络的丢包和网络不稳定所带来的其他问题，确保连接方能在时间范围内，关闭自己的连接 如何查看TIME-WAIT状态的链接数量？ ​ netstat -an |grep TIME_WAIT|wc -l 查看连接数等待time_wait状态连接数 为什么会TIME-WAIT过多？解决方法是怎样的？ ​ 可能原因： 高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接 ​ 解决：负载均衡服务器；Web服务器首先关闭来自负载均衡服务器的连接 1、OSI与TCP/IP 模型​ OSI七层：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层 ​ TCP/IP五层：物理层、数据链路层、网络层、传输层、应用层 2、常见网络服务分层​ 应用层：HTTP、SMTP、DNS、FTP ​ 传输层：TCP 、UDP ​ 网络层：ICMP 、IP、路由器、防火墙 ​ 数据链路层：网卡、网桥、交换机 ​ 物理层：中继器、集线器 3、TCP与UDP区别及场景 类型 特点 性能 应用过场景 首部字节 TCP 面向连接、可靠、字节流 传输效率慢、所需资源多 文件、邮件传输 20-60 UDP 无连接、不可靠、数据报文段 传输效率快、所需资源少 语音、视频、直播 8个字节 ​ 基于TCP的协议：HTTP、FTP、SMTP ​ 基于UDP的协议：RIP、DNS、SNMP 4、TCP滑动窗口，拥塞控制​ TCP通过：应用数据分割、对数据包进行编号、校验和、流量控制、拥塞控制、超时重传等措施保证数据的可靠传输； ​ 拥塞控制目的：为了防止过多的数据注入到网络中，避免网络中的路由器、链路过载 ​ 拥塞控制过程：TCP维护一个拥塞窗口，该窗口随着网络拥塞程度动态变化，通过慢开始、拥塞避免等算法减少网络拥塞的发生。 5、TCP粘包原因和解决方法​ TCP粘包是指：发送方发送的若干包数据到接收方接收时粘成一包 ​ 发送方原因： ​ TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量）： ​ 收集多个小分组，在一个确认到来时一起发送、导致发送方可能会出现粘包问题 ​ 接收方原因： ​ TCP将接收到的数据包保存在接收缓存里，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。 ​ 解决粘包问题： ​ 最本质原因在与接收对等方无法分辨消息与消息之间的边界在哪，通过使用某种方案给出边界，例如： 发送定长包。每个消息的大小都是一样的，接收方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。 包尾加上\\r\\n标记。FTP协议正是这么做的。但问题在于如果数据正文中也含有\\r\\n，则会误判为消息的边界。 包头加上包体长度。包头是定长的4个字节，说明了包体的长度。接收对等方先接收包体长度，依据包体长度来接收包体。 6、TCP、UDP报文格式​ TCP报文格式： ​ ​ 源端口号和目的端口号： ​ 用于寻找发端和收端应用进程。这两个值加上ip首部源端ip地址和目的端ip地址唯一确定一个tcp连接。 ​ 序号字段： ​ 序号用来标识从T C P发端向T C P收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节。如果将字节流看作在两个应用程序间的单向流动，则 T C P用序号对每个字节进行计数。序号是32 bit的无符号数，序号到达 2^32-1后又从0开始。 当建立一个新的连接时，SYN标志变1。序号字段包含由这个主机选择的该连接的初始序号ISN（Initial Sequence Number）。该主机要发送数据的第一个字节序号为这个ISN加1，因为SYN标志消耗了一个序号 ​ 确认序号： ​ 既然每个传输的字节都被计数，确认序号包含发送确认的一端所期望收到的下一个序号。因此，确认序号应当是上次已成功收到数据字节序号加 1。只有ACK标志为 1时确认序号字段才有效。发送ACK无需任何代价，因为 32 bit的确认序号字段和A C K标志一样，总是T C P首部的一部分。因此，我们看到一旦一个连接建立起来，这个字段总是被设置， ACK标志也总是被设置为1。TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据序号。 ​ 首都长度： ​ 首部长度给出首部中 32 bit字的数目。需要这个值是因为任选字段的长度是可变的。这个字段占4 bit，因此T C P最多有6 0字节的首部。然而，没有任选字段，正常的长度是 2 0字节。 ​ 标志字段：在T C P首部中有 6个标志比特。它们中的多个可同时被设置为1. URG紧急指针（u rgent pointer）有效 ACK确认序号有效。 PSH接收方应该尽快将这个报文段交给应用层。 RST重建连接。 SYN同步序号用来发起一个连接。这个标志和下一个标志将在第 1 8章介绍。 FIN发端完成发送任务。 ​ 窗口大小： ​ T C P的流量控制由连接的每一端通过声明的窗口大小来提供。窗口大小为字节数，起始于确认序号字段指明的值，这个值是接收端期望接收的字节。窗口大小是一个 16 bit字段，因而窗口大小最大为 65535字节。 ​ 检验和： ​ 检验和覆盖了整个的 T C P报文段：T C P首部和T C P数据。这是一个强制性的字段，一定是由发端计算和存储，并由收端进行验证。 ​ 紧急指针： ​ 只有当URG标志置1时紧急指针才有效。紧急指针是一个正的偏移量，和序号字段中的值相加表示紧急数据最后一个字节的序号。 T C P的紧急方式是发送端向另一端发送紧急数据的一种方式。 ​ 选项： ​ 最常见的可选字段是最长报文大小，又称为 MSS (Maximum Segment Size)。每个连接方通常都在通信的第一个报文段（为建立连接而设置 S Y N标志的那个段）中指明这个选项。它指明本端所能接收的最大长度的报文段。 ​ UDP报文格式： ​ ​ 端口号： ​ 用来表示发送和接受进程。由于 I P层已经把I P数据报分配给T C P或U D P（根据I P首部中协议字段值），因此T C P端口号由T C P来查看，而 U D P端口号由UDP来查看。T C P端口号与UDP端口号是相互独立的。 ​ 长度： ​ UDP长度字段指的是UDP首部和UDP数据的字节长度。该字段的最小值为 8字节（发送一份0字节的UDP数据报是 O K）。 ​ 检验和： ​ UDP检验和是一个端到端的检验和。它由发送端计算，然后由接收端验证。其目的是为了发现UDP首部和数据在发送端到接收端之间发生的任何改动。 ​ IP报文格式：普通的IP首部长为20个字节，除非含有可选项字段。 ​ ​ 4位版本： ​ 目前协议版本号是4，因此IP有时也称作IPV4. ​ 4位首部长度： ​ 首部长度指的是首部占32bit字的数目，包括任何选项。由于它是一个4比特字段，因此首部长度最长为60个字节。 ​ 服务类型（TOS）： ​ 服务类型字段包括一个3bit的优先权字段（现在已经被忽略），4bit的TOS子字段和1bit未用位必须置0。4bit的TOS分别代表：最小时延，最大吞吐量，最高可靠性和最小费用。4bit中只能置其中1比特。如果所有4bit均为0，那么就意味着是一般服务。 ​ 总长度： ​ 总长度字段是指整个IP数据报的长度，以字节为单位。利用首部长度和总长度字段，就可以知道IP数据报中数据内容的起始位置和长度。由于该字段长16bit，所以IP数据报最长可达65535字节。当数据报被分片时，该字段的值也随着变化。 ​ 标识字段： ​ 标识字段唯一地标识主机发送的每一份数据报。通常每发送一份报文它的值就会加1。 ​ 生存时间： ​ TTL（time-to-live）生存时间字段设置了数据报可以经过的最多路由器数。它指定了数据报的生存时间。TTL的初始值由源主机设置（通常为 3 2或6 4），一旦经过一个处理它的路由器，它的值就减去 1。当该字段的值为 0时，数据报就被丢弃，并发送 ICMP 报文通知源主机。 ​ 首部检验和： ​ 首部检验和字段是根据 I P首部计算的检验和码。它不对首部后面的数据进行计算。 ICMP、IGMP、UDP和TCP在它们各自的首部中均含有同时覆盖首部和数据检验和码。 ​ 以太网报文格式： ​ 目的地址和源地址： ​ 是指网卡的硬件地址（也叫MAC 地址），长度是48 位，是在网卡出厂时固化的。 ​ 数据： ​ 以太网帧中的数据长度规定最小46 字节，最大1500 字节，ARP 和RARP 数据包的长度不够46 字节，要在后面补填充位。最大值1500 称为以太网的最大传输单元（MTU），不同的网络类型有不同的MTU，如果一个数据包从以太网路由到拨号链路上，数据包度大于拨号链路的MTU了，则需要对数据包进行分片fragmentation）。ifconfig 命令的输出中也有“MTU:1500”。注意，MTU 个概念指数据帧中有效载荷的最大长度，不包括帧首部的长度。 HTTP协议1、HTTP协议1.0_1.1_2.0​ HTTP1.0：服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记录过去的请求（无状态） ​ HTTP1.1：KeepAlived长连接避免了连接建立和释放的开销；通过Content-Length来判断当前请求数据是否已经全部接受（有状态） ​ HTTP2.0：引入二进制数据帧和流的概念，其中帧对数据进行顺序标识；因为有了序列，服务器可以并行的传输数据。 ​ http1.0和http1.1的主要区别如下：​ 1、缓存处理：1.1添加更多的缓存控制策略（如：Entity tag，If-Match）​ 2、网络连接的优化：1.1支持断点续传​ 3、错误状态码的增多：1.1新增了24个错误状态响应码，丰富的错误码更加明确各个状态​ 4、Host头处理：支持Host头域，不在以IP为请求方标志​ 5、长连接：减少了建立和关闭连接的消耗和延迟。 ​ http1.1和http2.0的主要区别：​ 1、新的传输格式：2.0使用二进制格式，1.0依然使用基于文本格式​ 2、多路复用：连接共享，不同的request可以使用同一个连接传输（最后根据每个request上的id号组合成正常的请求）​ 3、header压缩：由于1.X中header带有大量的信息，并且得重复传输，2.0使用encoder来减少需要传输的hearder大小​ 4、服务端推送：同google的SPDUY（1.0的一种升级）一样 2、HTTP与HTTPS之间的区别​ HTTP与HTTPS之间的区别： HTTP HTTPS 默认端口80 HTTPS默认使用端口443 明文传输、数据未加密、安全性差 传输过程ssl加密、安全性较好 响应速度快、消耗资源少 响应速度较慢、消耗资源多、需要用到CA证书 ​ HTTPS链接建立的过程： ​ 1.首先客户端先给服务器发送一个请求 ​ 2.服务器发送一个SSL证书给客户端，内容包括：证书的发布机构、有效期、所有者、签名以及公钥 ​ 3.客户端对发来的公钥进行真伪校验，校验为真则使用公钥对对称加密算法以及对称密钥进行加密 ​ 4.服务器端使用私钥进行解密并使用对称密钥加密确认信息发送给客户端 ​ 5.随后客户端和服务端就使用对称密钥进行信息传输 ​ 对称加密算法： ​ 双方持有相同的密钥，且加密速度快，典型对称加密算法：DES、AES ​ 非对称加密算法： ​ 密钥成对出现（私钥、公钥），私钥只有自己知道，不在网络中传输；而公钥可以公开。相比对称加密速度较慢，典型的非对称加密算法有：RSA、DSA 3、Get和Post请求区别HTTP请求： 方法 描述 GET 向特定资源发送请求，查询数据，并返回实体 POST 向指定资源提交数据进行处理请求，可能会导致新的资源建立、已有资源修改 PUT 向服务器上传新的内容 HEAD 类似GET请求，返回的响应中没有具体的内容，用于获取报头 DELETE 请求服务器删除指定标识的资源 OPTIONS 可以用来向服务器发送请求来测试服务器的功能性 TRACE 回显服务器收到的请求，用于测试或诊断 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器 get和Post区别： GET POST 可见性 数据在URL中对所有人可见 数据不会显示在URL中 安全性 与post相比，get的安全性较差，因为所发送的数据是URL的一部分 安全，因为参数不会被保存在浏览器历史或web服务器日志中 数据长度 受限制，最长2kb 无限制 编码类型 application/x-www-form-urlencoded multipart/form-data 缓存 能被缓存 不能被缓存 4、HTTP常见响应状态码​ 100：Continue — 继续。客户端应继续其请求。 ​ 200：OK — 请求成功。一般用于GET与POST请求。 ​ 301：Moved Permanently — 永久重定向。 ​ 302：Found — 暂时重定向。 ​ 400：Bad Request — 客户端请求的语法错误，服务器无法理解。 ​ 403：Forbideen — 服务器理解请求客户端的请求，但是拒绝执行此请求。 ​ 404：Not Found — 服务器无法根据客户端的请求找到资源（网页）。 ​ 500：Internal Server Error — 服务器内部错误，无法完成请求。 ​ 502：Bad Gateway — 作为网关或者代理服务器尝试执行请求时，从远程服务器接收到了无效的响应。 5、重定向和转发区别​ 重定向：redirect： ​ 地址栏发生变化 ​ 重定向可以访问其他站点（服务器）的资源 ​ 重定向是两次请求。不能使用request对象来共享数据 ​ 转发：forward： ​ 转发地址栏路径不变 ​ 转发只能访问当前服务器下的资源 ​ 转发是一次请求，可以使用request对象共享数据 6、Cookie和Session区别。​ Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但两者有所区别： ​ Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。 ​ cookie不是很安全，别人可以分析存放在本地的COOKIE并进行欺骗,考虑到安全应当使用session。 ​ Cookie ⼀般⽤来保存⽤户信息，Session 的主要作⽤就是通过服务端记录⽤户的状态 浏览器输入URL过程​ 过程：DNS解析、TCP连接、发送HTTP请求、服务器处理请求并返回HTTP报文、浏览器渲染、结束 过程 使用的协议 1、浏览器查找域名DNS的IP地址DNS查找过程（浏览器缓存、路由器缓存、DNS缓存） DNS：获取域名对应的ip 2、根据ip建立TCP连接 TCP：与服务器建立连接 3、浏览器向服务器发送HTTP请求 HTTP：发送请求 4、服务器响应HTTP响应 HTTP 5、浏览器进行渲染 操作系统基础进程和线程的区别​ 进程：是资源分配的最小单位，一个进程可以有多个线程，多个线程共享进程的堆和方法区资源，不共享栈、程序计数器 ​ 线程：是任务调度和执行的最小单位，线程并行执行存在资源竞争和上下文切换的问题 ​ 协程：是一种比线程更加轻量级的存在，正如一个进程可以拥有多个线程一样，一个线程可以拥有多个协程。 1、进程间通信方式IPC管道pipe： ​ 亲缘关系使用匿名管道，非亲缘关系使用命名管道，管道遵循FIFO，半双工，数据只能单向通信； 信号： ​ 信号是一种比较复杂的通信方式，用户调用kill命令将信号发送给其他进程。 消息队列： ​ 消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等特点。 共享内存(share memory)： 使得多个进程可以可以直接读写同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。 由于多个进程共享一段内存，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。 信号量(Semaphores) ： ​ 信号量是⼀个计数器，⽤于多进程对共享数据的访问，这种通信⽅式主要⽤于解决与同步相关的问题并避免竞争条件。 套接字(Sockets) : ​ 简单的说就是通信的两⽅的⼀种约定，⽤套接字中的相关函数来完成通信过程。 2、用户态和核心态用户态：只能受限的访问内存，运行所有的应用程序 核心态：运行操作系统程序，cpu可以访问内存的所有数据，包括外围设备 为什么要有用户态和内核态： ​ 由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络 用户态切换到内核态的3种方式： ​ a. 系统调用 ​ 主动调用，系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 ​ b. 异常 ​ 当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，比如缺页异常，这时会触发切换内核态处理异常。 ​ c. 外围设备的中断 ​ 当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会由用户态到内核态的切换。 3、操作系统的进程空间​ 栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。 ​ 堆区（heap）— 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。 ​ 静态区（static）—存放全局变量和静态变量的存储 ​ 代码区(text)—存放函数体的二进制代码。 ​ 线程共享堆区、静态区 操作系统内存管理存管理方式：页式管理、段式管理、段页式管理 分段管理： ​ 将程序的地址空间划分为若干段（segment），如代码段，数据段，堆栈段；这样每个进程有一个二维地址空间，相互独立，互不干扰。段式管理的优点是：没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片） 分页管理： ​ 在页式存储管理中，将程序的逻辑地址划分为固定大小的页（page），而物理内存划分为同样大小的页框，程序加载时，可以将任意一页放入内存中任意一个页框，这些页框不必连续，从而实现了离散分离。页式存储管理的优点是：没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满） 段页式管理： ​ 段⻚式管理机制结合了段式管理和⻚式管理的优点。简单来说段⻚式管理机制就是把主存先分成若⼲段，每个段⼜分成若⼲⻚，也就是说 段⻚式管理机制 中段与段之间以及段的内部的都是离散的 1、页面置换算法FIFO、LRU置换算法：先进先出FIFO、最近最久未使用LRU、最佳置换算法OPT 先进先出FIFO: ​ 缺点：没有考虑到实际的页面使用频率，性能差、与通常页面使用的规则不符合，实际应用较少 最近最久未使用LRU: ​ 原理：选择最近且最久未使用的页面进行淘汰 ​ 优点：考虑到了程序访问的时间局部性，有较好的性能，实际应用也比较多 ​ 缺点：没有合适的算法，只有适合的算法，lFU、random都可以 /** * @program: Java * @description: LRU最近最久未使用置换算法，通过LinkedHashMap实现 * @author: Mr.Li * @create: 2020-07-17 10:29 **/ public class LRUCache &#123; private LinkedHashMap&lt;Integer,Integer&gt; cache; private int capacity; //容量大小 /** *初始化构造函数 * @param capacity */ public LRUCache(int capacity) &#123; cache = new LinkedHashMap&lt;&gt;(capacity); this.capacity = capacity; &#125; public int get(int key) &#123; //缓存中不存在此key，直接返回 if(!cache.containsKey(key)) &#123; return -1; &#125; int res = cache.get(key); cache.remove(key); //先从链表中删除 cache.put(key,res); //再把该节点放到链表末尾处 return res; &#125; public void put(int key,int value) &#123; if(cache.containsKey(key)) &#123; cache.remove(key); //已经存在，在当前链表移除 &#125; if(capacity == cache.size()) &#123; //cache已满，删除链表头位置 Set&lt;Integer&gt; keySet = cache.keySet(); Iterator&lt;Integer&gt; iterator = keySet.iterator(); cache.remove(iterator.next()); &#125; cache.put(key,value); //插入到链表末尾 &#125; &#125; /** * @program: Java * @description: LRU最近最久未使用置换算法，通过LinkedHashMap内部removeEldestEntry方法实现 * @author: Mr.Li * @create: 2020-07-17 10:59 **/ class LRUCache &#123; private Map&lt;Integer, Integer&gt; map; private int capacity; /** *初始化构造函数 * @param capacity */ public LRUCache(int capacity) &#123; this.capacity = capacity; map = new LinkedHashMap&lt;Integer, Integer&gt;(capacity, 0.75f, true) &#123; @Override protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; capacity; // 容量大于capacity 时就删除 &#125; &#125;; &#125; public int get(int key) &#123; //返回key对应的value值，若不存在，返回-1 return map.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; map.put(key, value); &#125; &#125; 最佳置换算法OPT: ​ 原理：每次选择当前物理块中的页面在未来长时间不被访问的或未来不再使用的页面进行淘汰 ​ 优点：具有较好的性能，可以保证获得最低的缺页率 ​ 缺点：过于理想化，但是实际上无法实现（没办法预知未来的页面） 2、死锁条件、解决方式。​ 死锁是指两个或两个以上进程在执行过程中，因争夺资源而造成的下相互等待的现象； ​ 死锁的条件： ​ 互斥条件：进程对所分配到的资源不允许其他进程访问，若其他进程访问该资源，只能等待至占有该资源的进程释放该资源； ​ 请求与保持条件：进程获得一定的资源后，又对其他资源发出请求，阻塞过程中不会释放自己已经占有的资源 ​ 非剥夺条件：进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用后自己释放 ​ 循环等待条件：系统中若干进程组成环路，环路中每个进程都在等待相邻进程占用的资源 ​ 解决方法：破坏死锁的任意一条件 ​ 乐观锁，破坏资源互斥条件，CAS ​ 资源一次性分配，从而剥夺请求和保持条件、tryLock ​ 可剥夺资源：即当进程新的资源未得到满足时，释放已占有的资源，从而破坏不可剥夺的条件，数据库deadlock超时 ​ 资源有序分配法：系统给每类资源赋予一个序号，每个进程按编号递增的请求资源，从而破坏环路等待的条件，转账场景 Java基础面向对象三大特性特性：封装、继承、多态 ​ 封装：对抽象的事物抽象化成一个对象，并对其对象的属性私有化，同时提供一些能被外界访问属性的方法； ​ 继承：子类扩展新的数据域或功能，并复用父类的属性与功能，单继承，多实现； ​ 多态：通过继承（多个⼦类对同⼀⽅法的重写）、也可以通过接⼝（实现接⼝并覆盖接⼝） 1、Java与C++区别​ 不同点：c++支持多继承，并且有指针的概念，由程序员自己管理内存；Java是单继承，可以用接口实现多继承，Java 不提供指针来直接访问内存，程序内存更加安全，并且Java有JVM⾃动内存管理机制，不需要程序员⼿动释放⽆⽤内存 2、多态实现原理多态的底层实现是动态绑定，即在运行时才把方法调用与方法实现关联起来。 静态绑定与动态绑定： ​ 一种是在编译期确定，被称为静态分派，比如方法的重载； ​ 一种是在运行时确定，被称为动态分派，比如方法的覆盖（重写）和接口的实现。 多态的实现 ​ 虚拟机栈中会存放当前方法调用的栈帧（局部变量表、操作栈、动态连接 、返回地址）。多态的实现过程，就是方法调用动态分派的过程，如果子类覆盖了父类的方法，则在多态调用中，动态绑定过程会首先确定实际类型是子类，从而先搜索到子类中的方法。这个过程便是方法覆盖的本质。 3、static和final关键字static：可以修饰属性、方法 ​ static修饰属性： ​ 类级别属性，所有对象共享一份，随着类的加载而加载（只加载一次），先于对象的创建；可以使用类名直接调用。 ​ static修饰方法： ​ 随着类的加载而加载；可以使用类名直接调用；静态方法中，只能调用静态的成员，不可用this； final：关键字主要⽤在三个地⽅：变量、⽅法、类。 ​ final修饰变量： ​ 如果是基本数据类型的变量，则其数值⼀旦在初始化之后便不能更改； ​ 如果是引⽤类型的变量，则在对其初始化之后便不能再让其指向另⼀个对象。 ​ final修饰方法： ​ 把⽅法锁定，以防任何继承类修改它的含义（重写）；类中所有的 private ⽅法都隐式地指定为 final。 ​ final修饰类： ​ final 修饰类时，表明这个类不能被继承。final 类中的所有成员⽅法都会被隐式地指定为 final ⽅法。 一个类不能被继承，除了final关键字之外，还有可以私有化构造器。（内部类无效） 4、抽象类和接口抽象类：包含抽象方法的类，即使用abstract修饰的类；抽象类只能被继承，所以不能使用final修饰，抽象类不能被实例化， 接口：接口是一个抽象类型，是抽象方法的集合，接口支持多继承，接口中定义的方法，默认是public abstract修饰的抽象方法 相同点： ​ ① 抽象类和接口都不能被实例化 ​ ② 抽象类和接口都可以定义抽象方法，子类/实现类必须覆写这些抽象方法 不同点： ​ ① 抽象类有构造方法，接口没有构造方法 ​ ③抽象类可以包含普通方法，接口中只能是public abstract修饰抽象方法（Java8之后可以） ​ ③ 抽象类只能单继承，接口可以多继承 ​ ④ 抽象类可以定义各种类型的成员变量，接口中只能是public static final修饰的静态常量 抽象类的使用场景： ​ 既想约束子类具有共同的行为（但不再乎其如何实现），又想拥有缺省的方法，又能拥有实例变量 接口的应用场景： ​ 约束多个实现类具有统一的行为，但是不在乎每个实现类如何具体实现；实现类中各个功能之间可能没有任何联系 5、泛型以及泛型擦除参考：https://blog.csdn.net/baoyinwang/article/details/107341997 泛型： ​ 泛型的本质是参数化类型。这种参数类型可以用在类、接口和方法的创建中，分别称为泛型类、泛型接口和泛型方法。 泛型擦除： ​ Java的泛型是伪泛型，使用泛型的时候加上类型参数，在编译器编译生成的字节码的时候会去掉，这个过程成为类型擦除。 ​ 如List等类型，在编译之后都会变成 List。JVM 看到的只是 List，而由泛型附加的类型信息对 JVM 来说是不可见的。 可以通过反射添加其它类型元素 6、反射原理以及使用场景Java反射： ​ 是指在运行状态中，对于任意一个类都能够知道这个类所有的属性和方法；并且都能够调用它的任意一个方法； 反射原理： ​ 反射首先是能够获取到Java中的反射类的字节码，然后将字节码中的方法，变量，构造函数等映射成 相应的 Method、Filed、Constructor 等类 ​ 如何得到Class的实例: 1.类名.class(就是一份字节码) 2.Class.forName(String className);根据一个类的全限定名来构建Class对象 3.每一个对象多有getClass()方法:obj.getClass();返回对象的真实类型 使用场景： 开发通用框架 - 反射最重要的用途就是开发各种通用框架。很多框架（比如 Spring）都是配置化的（比如通过 XML 文件配置 JavaBean、Filter 等），为了保证框架的通用性，需要根据配置文件运行时动态加载不同的对象或类，调用不同的方法。 动态代理 - 在切面编程（AOP）中，需要拦截特定的方法，通常，会选择动态代理方式。这时，就需要反射技术来实现了。 JDK：spring默认动态代理，需要实现接口 CGLIB：通过asm框架序列化字节流，可配置，性能差 自定义注解 - 注解本身仅仅是起到标记作用，它需要利用反射机制，根据注解标记去调用注解解释器，执行行为。 7、Java异常体系​ Throwable 是 Java 语言中所有错误或异常的超类。下一层分为 Error 和 Exception Error ： ​ 是指 java 运行时系统的内部错误和资源耗尽错误。应用程序不会抛出该类对象。如果出现了这样的错误，除了告知用户，剩下的就是尽力使程序安全的终止。 Exception 包含：RuntimeException 、CheckedException 编程错误可以分成三类：语法错误、逻辑错误和运行错误。 语法错误（也称编译错误）是在编译过程中出现的错误，由编译器检查发现语法错误 逻辑错误指程序的执行结果与预期不符，可以通过调试定位并发现错误的原因 运行错误是引起程序非正常终端的错误，需要通过异常处理的方式处理运行错误 RuntimeException： 运行时异常，程序应该从逻辑角度尽可能避免这类异常的发生。 ​ 如 NullPointerException 、 ClassCastException ； CheckedException：受检异常，程序使用trycatch进行捕捉处理 ​ 如IOException、SQLException、NotFoundException； 数据结构 1、ArrayList和LinkedListArrayList： ​ 底层基于数组实现，支持对元素进行快速随机访问，适合随机查找和遍历，不适合插入和删除。（提一句实际上）​ 默认初始大小为10，当数组容量不够时，会触发扩容机制（扩大到当前的1.5倍），需要将原来数组的数据复制到新的数组中；当从 ArrayList 的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。 LinkedList： ​ 底层基于双向链表实现，适合数据的动态插入和删除；​ 内部提供了 List 接口中没有定义的方法，用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。（比如jdk官方推荐使用基于linkedList的Deque进行堆栈操作） ArrayList与LinkedList区别： ​ 都是线程不安全的，ArrayList 适用于查找的场景，LinkedList 适用于增加、删除多的场景 实现线程安全： ​ 可以使用原生的Vector，或者是Collections.synchronizedList(List list)函数返回一个线程安全的ArrayList集合。​ 建议使用concurrent并发包下的CopyOnWriteArrayList的。 ​ ①Vector: 底层通过synchronize修饰保证线程安全，效率较差 ​ ②CopyOnWriteArrayList：写时加锁，使用了一种叫写时复制的方法；读操作是可以不用加锁的 ​ 2、List遍历快速和安全失败①普通for循环遍历List删除指定元素 for(int i=0; i &lt; list.size(); i++)&#123; if(list.get(i) == 5) list.remove(i); &#125; ② 迭代遍历,用list.remove(i)方法删除元素 Iterator&lt;Integer&gt; it = list.iterator(); while(it.hasNext())&#123; Integer value = it.next(); if(value == 5)&#123; list.remove(value); &#125; &#125; ③foreach遍历List删除元素 for(Integer i:list)&#123; if(i==3) list.remove(i); &#125; fail—fast：快速失败 ​ 当异常产生时，直接抛出异常，程序终止; ​ fail-fast主要是体现在当我们在遍历集合元素的时候，经常会使用迭代器，但在迭代器遍历元素的过程中，如果集合的结构（modCount）被改变的话，就会抛出异常ConcurrentModificationException，防止继续遍历。这就是所谓的快速失败机制。 fail—safe：安全失败 采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。由于在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发ConcurrentModificationException。 缺点：基于拷贝内容的优点是避免了ConcurrentModificationException，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 3、详细介绍HashMap角度：数据结构+扩容情况+put查找的详细过程+哈希函数+容量为什么始终都是2^N，JDK1.7与1.8的区别。 参考：https://www.jianshu.com/p/9fe4cb316c05 数据结构： ​ HashMap在底层数据结构上采用了数组＋链表＋红黑树，通过散列映射来存储键值对数据 扩容情况： ​ 默认的负载因子是0.75，如果数组中已经存储的元素个数大于数组长度的75%，将会引发扩容操作。 ​ 【1】创建一个长度为原来数组长度两倍的新数组。 ​ 【2】1.7采用Entry的重新hash运算，1.8采用高于与运算。 put操作步骤： ​ ​ 1、判断数组是否为空，为空进行初始化; ​ 2、不为空，则计算 key 的 hash 值，通过(n - 1) &amp; hash计算应当存放在数组中的下标 index; ​ 3、查看 table[index] 是否存在数据，没有数据就构造一个Node节点存放在 table[index] 中； ​ 4、存在数据，说明发生了hash冲突(存在二个节点key的hash值一样), 继续判断key是否相等，相等，用新的value替换原数据； ​ 5、若不相等，判断当前节点类型是不是树型节点，如果是树型节点，创造树型节点插入红黑树中； ​ 6、若不是红黑树，创建普通Node加入链表中；判断链表长度是否大于 8，大于则将链表转换为红黑树； ​ 7、插入完成之后判断当前节点数是否大于阈值，若大于，则扩容为原数组的二倍 哈希函数： ​ 通过hash函数（优质因子31循环累加）先拿到 key 的hashcode，是一个32位的值，然后让hashcode的高16位和低16位进行异或操作。该函数也称为扰动函数，做到尽可能降低hash碰撞，通过尾插法进行插入。 容量为什么始终都是2^N： ​ 先做对数组的⻓度取模运算，得到的余数才能⽤来要存放的位置也就是对应的数组下标。这个数组下标的计算⽅法是“ (n - 1) &amp; hash ”。（n代表数组⻓度）。方便数组的扩容和增删改时的取模。 JDK1.7与1.8的区别： JDK1.7 HashMap： ​ 底层是 数组和链表 结合在⼀起使⽤也就是链表散列。如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。扩容翻转时顺序不一致使用头插法会产生死循环，导致cpu100% JDK1.8 HashMap： ​ 底层数据结构上采用了数组＋链表＋红黑树；当链表⻓度⼤于阈值（默认为 8-泊松分布），数组的⻓度大于 64时，链表将转化为红⿊树，以减少搜索时间。（解决了tomcat臭名昭著的url参数dos攻击问题） **4、ConcurrentHashMap **​ 可以通过ConcurrentHashMap 和 Hashtable来实现线程安全；Hashtable 是原始API类，通过synchronize同步修饰，效率低下；ConcurrentHashMap 通过分段锁实现，效率较比Hashtable要好； ConcurrentHashMap的底层实现： ​ JDK1.7的 ConcurrentHashMap 底层采⽤ 分段的数组+链表 实现；采用 分段锁（Sagment） 对整个桶数组进⾏了分割分段(Segment默认16个)，每⼀把锁只锁容器其中⼀部分数据，多线程访问容器⾥不同数据段的数据，就不会存在锁竞争，提⾼并发访问率。 ​ JDK1.8的 ConcurrentHashMap 采⽤的数据结构跟HashMap1.8的结构⼀样，数组+链表/红⿊树；摒弃了Segment的概念，⽽是直接⽤ Node 数组+链表+红⿊树的数据结构来实现，通过并发控制 synchronized 和CAS来操作保证线程的安全。 5、序列化和反序列化​ 序列化的意思就是将对象的状态转化成字节流，以后可以通过这些值再生成相同状态的对象。对象序列化是对象持久化的一种实现方法，它是将对象的属性和方法转化为一种序列化的形式用于存储和传输。反序列化就是根据这些保存的信息重建对象的过程。 序列化：将java对象转化为字节序列的过程。 反序列化：将字节序列转化为java对象的过程。 优点： ​ a、实现了数据的持久化，通过序列化可以把数据永久地保存到硬盘上（通常存放在文件里）Redis的RDB ​ b、利用序列化实现远程通信，即在网络上传送对象的字节序列。 Google的protoBuf 反序列化失败的场景： ​ 序列化ID：serialVersionUID不一致的时候，导致反序列化失败 6、StringString 使用数组存储内容，数组使用 final 修饰，因此 String 定义的字符串的值也是不可变的 StringBuffer 对方法加了同步锁，线程安全，效率略低于 StringBuilder 设计模式与原则1、单例模式​ 某个类只能生成一个实例，该实例全局访问，例如Spring容器里一级缓存里的单例池。 优点： ​ 唯一访问：如生成唯一序列化的场景、或者spring默认的bean类型。 ​ 提高性能：频繁实例化创建销毁或者耗时耗资源的场景，如连接池、线程池。 缺点： ​ 不适合有状态且需变更的 实现方式： ​ 饿汉式：线程安全速度快 ​ 懒汉式：双重检测锁，第一次减少锁的开销、第二次防止重复、volatile防止重排序导致实例化未完成 ​ 静态内部类：线程安全利用率高 ​ 枚举：effictiveJAVA推荐，反射也无法破坏 2、工厂模式​ 定义一个用于创建产品的接口，由子类决定生产何种产品。 优点：解耦：提供参数即可获取产品，通过配置文件可以不修改代码增加具体产品。 缺点：每增加一个产品就得新增一个产品类 3、抽象工厂模式​ 提供一个接口，用于创建相关或者依赖对象的家族，并由此进行约束。 优点：可以在类的内部对产品族进行约束 缺点：假如产品族中需要增加一个新的产品，则几乎所有的工厂类都需要进行修改。 面试题构造方法构造方法可以被重载，只有当类中没有显性声明任何构造方法时，才会有默认构造方法。 构造方法没有返回值，构造方法的作用是创建新对象。 初始化块静态初始化块的优先级最高，会最先执行，在非静态初始化块之前执行。 静态初始化块会在类第一次被加载时最先执行，因此在 main 方法之前。 This关键字 this 代表当前对象的引用。当前对象指的是调用类中的属性或方法的对象 关键字 this 不可以在静态方法中使用。静态方法不依赖于类的具体对象的引用 重写和重载的区别重载指在同一个类中定义多个方法，这些方法名称相同，签名不同。 重写指在子类中的方法的名称和签名都和父类相同，使用override注解 Object类方法toString 默认是个指针，一般需要重写 equals 比较对象是否相同，默认和==功能一致 hashCode 散列码，equals则hashCode相同，所以重写equals必须重写hashCode **finalize ** 用于垃圾回收之前做的遗嘱，默认空，子类需重写 clone 深拷贝，类需实现cloneable的接口 getClass 反射获取对象元数据，包括类名、方法、 notify、wait 用于线程通知和唤醒 基本数据类型和包装类 类型 缓存范围 Byte,Short,Integer,Long [-128, 127] Character [0, 127] Boolean [false, true] 二、JVM篇JVM内存划分1、JVM运行时数据区域​ 堆、方法区（元空间）、虚拟机栈、本地方法栈、程序计数器 Heap(堆)： ​ 对象的实例以及数组的内存都是要在堆上进行分配的，堆是线程共享的一块区域，用来存放对象实例，也是垃圾回收（GC）的主要区域；开启逃逸分析后，某些未逃逸的对象可以通过标量替换的方式在栈中分配 ​ 堆细分：新生代、老年代，对于新生代又分为：Eden区和Surviver1和Surviver2区； 方法区： ​ 对于JVM的方法区也可以称之为永久区，它储存的是已经被java虚拟机加载的类信息、常量、静态变量；Jdk1.8以后取消了方法区这个概念，称之为元空间（MetaSpace）； ​ 当应用中的 Java 类过多时，比如 Spring 等一些使用动态代理的框架生成了很多类，如果占用空间超出了我们的设定值，就会发生元空间溢出 虚拟机栈： ​ 虚拟机栈是线程私有的，他的生命周期和线程的生命周期是一致的。里面装的是一个一个的栈帧，每一个方法在执行的时候都会创建一个栈帧，栈帧中用来存放（局部变量表、操作数栈 、动态链接 、返回地址）；在Java虚拟机规范中，对此区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将会抛出StackOverflowError异常；如果虚拟机栈动态扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 局部变量表：局部变量表是一组变量值存储空间，用来存放方法参数、方法内部定义的局部变量。底层是变量槽（variable slot） 操作数栈：是用来记录一个方法在执行的过程中，字节码指令向操作数栈中进行入栈和出栈的过程。大小在编译的时候已经确定了，当一个方法刚开始执行的时候，操作数栈中是空发的，在方法执行的过程中会有各种字节码指令往操作数栈中入栈和出栈。 动态链接：因为字节码文件中有很多符号的引用，这些符号引用一部分会在类加载的解析阶段或第一次使用的时候转化成直接引用，这种称为静态解析；另一部分会在运行期间转化为直接引用，称为动态链接。 返回地址（returnAddress）：类型（指向了一条字节码指令的地址） JIT即时编译器（Just In Time Compiler），简称 JIT 编译器: 为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，比如锁粗化等 本地方法栈： ​ 本地方法栈和虚拟机栈类似，不同的是虚拟机栈服务的是Java方法，而本地方法栈服务的是Native方法。在HotSpot虚拟机实现中是把本地方法栈和虚拟机栈合二为一的，同理它也会抛出StackOverflowError和OOM异常。 PC程序计数器： ​ PC，指的是存放下一条指令的位置的一个指针。它是一块较小的内存空间，且是线程私有的。由于线程的切换，CPU在执行的过程中，需要记住原线程的下一条指令的位置，所以每一个线程都需要有自己的PC。 2、堆内存分配策略 对象优先分配在Eden区，如果Eden区没有足够的空间进行分配时，虚拟机执行一次MinorGC。而那些无需回收的存活对象，将会进到 Survivor 的 From 区（From 区内存不足时，直接进入 Old 区）。 大对象直接进入老年代（需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄（Age Count）计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，直到达到阀值（默认15次），对象进入老年区。 （动态对象年龄判定：程序从年龄最小的对象开始累加，如果累加的对象大小，大于幸存区的一半，则将当前的对象 age 作为新的阈值，年龄大于此阈值的对象则直接进入老年代） 每次进行Minor GC或者大对象直接进入老年区时，JVM会计算所需空间大小如小于老年区的剩余值大小，则进行一次Full GC。 3、创建一个对象的步骤步骤：类加载检查、分配内存、初始化零值、设置对象头、执行init方法 ①类加载检查： ​ 虚拟机遇到 new 指令时，⾸先去检查是否能在常量池中定位到这个类的符号引⽤，并且检查这个符号引⽤代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执⾏相应的类加载过程。 ②分配内存： ​ 在类加载检查通过后，接下来虚拟机将为新⽣对象分配内存，分配⽅式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配⽅式由 Java 堆是否规整决定，⽽Java堆是否规整⼜由所采⽤的垃圾收集器是否带有压缩整理功能决定。 ③初始化零值： ​ 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值，这⼀步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使⽤，程序能访问到这些字段的数据类型所对应的零值。 ④设置对象头： ​ 初始化零值完成之后，虚拟机要对对象进⾏必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运⾏状态的不同，如是否启⽤偏向锁等，对象头会有不同的设置⽅式。 ⑤执⾏ init ⽅法： ​ 从虚拟机的视⻆来看，⼀个新的对象已经产⽣了，但从Java 程序的视⻆来看， ⽅法还没有执⾏，所有的字段都还为零。所以⼀般来说（除循环依赖），执⾏ new 指令之后会接着执⾏ ⽅法，这样⼀个真正可⽤的对象才算产⽣出来。 4、对象引用普通的对象引用关系就是强引用。 软引用用于维护一些可有可无的对象。只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。 弱引用对象相比软引用来说，要更加无用一些，它拥有更短的生命周期，当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 虚引用是一种形同虚设的引用，在现实场景中用的不是很多，它主要用来跟踪对象被垃圾回收的活动。 JVM类加载过程过程：加载、验证、准备、解析、初始化 加载阶段： ​ 1.通过一个类的全限定名来获取定义此类的二进制字节流。 ​ 2.将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 ​ 3.在Java堆中生成一个代表这个类的java.lang.class对象，作为方法区这些数据的访问入口。 验证阶段： ​ 1.文件格式验证（是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理） ​ 2.元数据验证（对字节码描述的信息进行语意分析，以保证其描述的信息符合Java语言规范要求） ​ 3.字节码验证（保证被校验类的方法在运行时不会做出危害虚拟机安全的行为） ​ 4.符号引用验证（虚拟机将符号引用转化为直接引用时，解析阶段中发生） 准备阶段： ​ 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段。将对象初始化为“零”值 解析阶段： ​ 解析阶段时虚拟机将常量池内的符号引用替换为直接引用的过程。 ​ 字符串常量池：堆上，默认class文件的静态常量池 ​ 运行时常量池：在方法区，属于元空间 初始化阶段： ​ 初始化阶段时加载过程的最后一步，而这一阶段也是真正意义上开始执行类中定义的Java程序代码。 1、双亲委派机制​ 每⼀个类都有⼀个对应它的类加载器。系统中的 ClassLoder 在协同⼯作的时候会默认使⽤ 双亲委派模型 。即在类加载的时候，系统会⾸先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，⾸先会把该请求委派该⽗类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当⽗类加载器⽆法处理时，才由⾃⼰来处理。当⽗类加载器为null时，会使⽤启动类加载器 BootstrapClassLoader 作为⽗类加载器。 使用好处： ​ 此机制保证JDK核心类的优先加载；使得Java程序的稳定运⾏，可以避免类的重复加载，也保证了 Java 的核⼼ API 不被篡改。如果不⽤没有使⽤双亲委派模型，⽽是每个类加载器加载⾃⼰的话就会出现⼀些问题，⽐如我们编写⼀个称为 java.lang.Object 类的话，那么程序运⾏的时候，系统就会出现多个不同的Object 类。 破坏双亲委派机制： 可以⾃⼰定义⼀个类加载器，重写loadClass方法； Tomcat 可以加载自己目录下的 class 文件，并不会传递给父类的加载器； Java 的 SPI，发起者 BootstrapClassLoader 已经是最上层了，它直接获取了 AppClassLoader 进行驱动加载，和双亲委派是相反的。 2、tomcat的类加载机制步骤： 先在本地cache查找该类是否已经加载过，看看 Tomcat 有没有加载过这个类。 如果Tomcat 没有加载过这个类，则从系统类加载器的cache中查找是否加载过。 如果没有加载过这个类，尝试用ExtClassLoader类加载器类加载，重点来了，这里并没有首先使用 AppClassLoader 来加载类。这个Tomcat 的 WebAPPClassLoader 违背了双亲委派机制，直接使用了 ExtClassLoader来加载类。这里注意 ExtClassLoader 双亲委派依然有效，ExtClassLoader 就会使用 Bootstrap ClassLoader 来对类进行加载，保证了 Jre 里面的核心类不会被重复加载。 比如在 Web 中加载一个 Object 类。WebAppClassLoader → ExtClassLoader → Bootstrap ClassLoader，这个加载链，就保证了 Object 不会被重复加载。 如果 BoostrapClassLoader，没有加载成功，就会调用自己的 findClass 方法由自己来对类进行加载，findClass 加载类的地址是自己本 web 应用下的 class。 加载依然失败，才使用 AppClassLoader 继续加载。 都没有加载成功的话，抛出异常。 总结一下以上步骤，WebAppClassLoader 加载类的时候，故意打破了JVM 双亲委派机制，绕开了 AppClassLoader，直接先使用 ExtClassLoader 来加载类。 JVM垃圾回收1、存活算法和两次标记过程引用计数法： ​ 给对象添加一个引用计数器，每当由一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 ​ 优点：实现简单，判定效率也很高 ​ 缺点：他很难解决对象之间相互循环引用的问题，基本上被抛弃 可达性分析法： ​ 通过一系列的成为“GC Roots”(活动线程相关的各种引用，虚拟机栈帧引用，静态变量引用，JNI引用)的对象作为起始点，从这些节点ReferenceChains开始向下搜索，搜索所走过的路径成为引用链，当一个对象到GC ROOTS没有任何引用链相连时，则证明此对象时不可用的； 两次标记过程： ​ 对象被回收之前，该对象的finalize()方法会被调用；两次标记，即第一次标记不在“关系网”中的对象。第二次的话就要先判断该对象有没有实现finalize()方法了，如果没有实现就直接判断该对象可回收；如果实现了就会先放在一个队列中，并由虚拟机建立的一个低优先级的线程去执行它，随后就会进行第二次的小规模标记，在这次被标记的对象就会真正的被回收了。 2、垃圾回收算法垃圾回收算法：复制算法、标记清除、标记整理、分代收集 复制算法：(young) ​ 将内存分为⼤⼩相同的两块，每次使⽤其中的⼀块。当这⼀块的内存使⽤完后，就将还存活的对象复制到另⼀块去，然后再把使⽤的空间⼀次清理掉。这样就使每次的内存回收都是对内存区间的⼀半进⾏回收； ​ 优点：实现简单，内存效率高，不易产生碎片 ​ 缺点：内存压缩了一半，倘若存活对象多，Copying 算法的效率会大大降低 标记清除：(cms) ​ 标记出所有需要回收的对象，在标记完成后统⼀回收所有被标记的对象 ​ 缺点：效率低，标记清除后会产⽣⼤量不连续的碎⽚，需要预留空间给分配阶段的浮动垃圾 标记整理：(old) ​ 标记过程仍然与“标记-清除”算法⼀样，再让所有存活的对象向⼀端移动，然后直接清理掉端边界以外的内存；解决了产生大量不连续碎片问题 分代收集： ​ 根据各个年代的特点选择合适的垃圾收集算法。 ​ 新生代采用复制算法，新生代每次垃圾回收都要回收大部分对象，存活对象较少，即要复制的操作比较少，一般将新生代划分为一块较大的 Eden 空间和两个较小的 Survivor 空间(From Space, To Space)，每次使用Eden 空间和其中的一块 Survivor 空间，当进行回收时，将该两块空间中还存活的对象复制到另一块 Survivor 空间中。 ​ 老年代的对象存活⼏率是⽐较⾼的，⽽且没有额外的空间对它进⾏分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进⾏垃圾收集。 Safepoint 当发生 GC 时，用户线程必须全部停下来，才可以进行垃圾回收，这个状态我们可以认为 JVM 是安全的（safe），整个堆的状态是稳定的。如果在 GC 前，有线程迟迟进入不了 safepoint，那么整个 JVM 都在等待这个阻塞的线程，造成了整体 GC 的时间变长 MinorGC、MajorGC、FullGCMinorGC 在年轻代空间不足的时候发生， MajorGC 指的是老年代的 GC，出现 MajorGC 一般经常伴有 MinorGC。 FullGC 1、当老年代无法再分配内存的时候；2、元空间不足的时候；3、显示调用 System.gc 的时候。另外，像 CMS 一类的垃圾回收器，在 MinorGC 出现 promotion failure 的时候也会发生 FullGC。 对象优先在 Eden 区分配 大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 大对象直接进入老年代 大对象是指需要连续内存空间的对象，比如很长的字符串以及数组。老年代直接分配的目的是避免在 Eden 区和 Survivor 区之间出现大量内存复制。 长期存活的对象进入老年代 虚拟机给每个对象定义了年龄计数器，对象在 Eden 区出生之后，如果经过一次 Minor GC 之后，将进入 Survivor 区，同时对象年龄变为 1，增加到一定阈值时则进入老年代（阈值默认为 15） 动态对象年龄判定 为了能更好地适应不同程序的内存状况，虚拟机并不总是要求对象的年龄必须达到阈值才能进入老年代。如果在 Survivor 区中相同年龄的所有对象的空间总和大于 Survivor 区空间的一半，则年龄大于或等于该年龄的对象直接进入老年代。 空间分配担保 在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的空间总和，如果这个条件成立，那么 Minor GC 可以确保是安全的。如果不成立则进行 Full GC。 3、垃圾收集器 ​ JDK3：Serial Parnew 关注效率 Serial： ​ Serial 是一个单线程的收集器，它不但只会使用一个 CPU 或一条线程去完成垃圾收集工作，并且在进行垃圾收集的同时，必须暂停其他所有的工作线程，直到垃圾收集结束。适合用于客户端垃圾收集器。 Parnew： ​ ParNew 垃圾收集器其实是 Serial 收集器的多线程版本，也使用复制算法，除了使用多线程进行垃圾收集之外，其余的行为和 Serial 收集器完全一样，ParNew 垃圾收集器在垃圾收集过程中同样也要暂停所有其他的工作线程。 ​ JDK5：parallel Scavenge+（Serial old/parallel old）关注吞吐量 parallel Scavenge：(关注吞吐量) ​ Parallel Scavenge收集器关注点是吞吐量（⾼效率的利⽤CPU）。CMS等垃圾收集器的关注点更多的是⽤户线程的停顿时间（提⾼⽤户体验）；高吞吐量可以最高效率地利用 CPU 时间，尽快地完成程序的运算任务，主要适用于在后台运算而不需要太多交互的任务。 Serial old： Serial收集器的⽼年代版本，它同样是⼀个单线程收集器，使用标记-整理算法。主要有两个用途： 在 JDK1.5 之前版本中与新生代的 Parallel Scavenge 收集器搭配使用。 作为年老代中使用 CMS 收集器的后备垃圾收集方案。 parallel old： ​ Parallel Scavenge收集器的⽼年代版本。使⽤多线程和“标记-整理”算法。 JDK8-CMS：（关注最短垃圾回收停顿时间） ​ CMS收集器是一种年老代垃圾收集器，其最主要目标是获取最短垃圾回收停顿时间，和其他年老代使用标记-整理算法不同，它使用多线程的标记-清除算法。最短的垃圾收集停顿时间可以为交互比较高的程序提高用户体验。CMS 工作机制相比其他的垃圾收集器来说更复杂，整个过程分为以下 4 个阶段： ​ 初始标记：只是标记一下 GC Roots 能直接关联的对象，速度很快，STW。 ​ 并发标记：进行 ReferenceChains跟踪的过程，和用户线程一起工作，不需要暂停工作线程。 ​ 重新标记：为了修正在并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，STW。 ​ 并发清除：清除 GC Roots 不可达对象，和用户线程一起工作，不需要暂停工作线程。 ​ 由于耗时最长的并发标记和并发清除过程中，垃圾收集线程可以和用户现在一起并发工作，所以总体上来看CMS 收集器的内存回收和用户线程是一起并发地执行。 ​ 优点：并发收集、低停顿 ​ 缺点：对CPU资源敏感；⽆法处理浮动垃圾；使⽤“标记清除”算法，会导致⼤量空间碎⽚产⽣。 JDK9-G1：（精准控制停顿时间，避免垃圾碎片） ​ 是⼀款⾯向服务器的垃圾收集器,主要针对配备多颗处理器及⼤容量内存的机器.以极⾼概率满⾜GC停顿时间要求的同时,还具备⾼吞吐量性能特征；相比与 CMS 收集器，G1 收集器两个最突出的改进是： ​ 【1】基于标记-整理算法，不产生内存碎片。 ​ 【2】可以非常精确控制停顿时间，在不牺牲吞吐量前提下，实现低停顿垃圾回收。 ​ G1 收集器避免全区域垃圾收集，它把堆内存划分为大小固定的几个独立区域，并且跟踪这些区域的垃圾收集进度，同时在后台维护一个优先级列表，每次根据所允许的收集时间，优先回收垃圾最多的区域。区域划分和优先级区域回收机制，确保 G1 收集器可以在有限时间获得最高的垃圾收集效率。 初始标记：Stop The World，仅使用一条初始标记线程对GC Roots关联的对象进行标记 并发标记：使用一条标记线程与用户线程并发执行。此过程进行可达性分析，速度很慢 最终标记：Stop The World，使用多条标记线程并发执行 筛选回收：回收废弃对象，此时也要 Stop The World，并使用多条筛选回收线程并发执行 **JDK11-ZGC:**（在不关注容量的情况获取最小停顿时间5TB/10ms） ​ 着色笔技术：加快标记过程 ​ 读屏障：解决GC和应用之间并发导致的STW问题 支持 TB 级堆内存（最大 4T， JDK13 最大16TB） 最大 GC 停顿 10ms 对吞吐量影响最大，不超过 15% 4、配置垃圾收集器 首先是内存大小问题，基本上每一个内存区域我都会设置一个上限，来避免溢出问题，比如元空间。 通常，堆空间我会设置成操作系统的 2/3，超过 8GB 的堆，优先选用 G1 然后我会对 JVM 进行初步优化，比如根据老年代的对象提升速度，来调整年轻代和老年代之间的比例 依据系统容量、访问延迟、吞吐量等进行专项优化，我们的服务是高并发的，对 STW 的时间敏感 我会通过记录详细的 GC 日志，来找到这个瓶颈点，借用 GCeasy 这样的日志分析工具，定位问题 4、JVM性能调优对应进程的JVM状态以定位问题和解决问题并作出相应的优化 常用命令：jps、jinfo、jstat、jstack、jmap jps：查看java进程及相关信息 jps -l 输出jar包路径，类全名 jps -m 输出main参数 jps -v 输出JVM参数 jinfo：查看JVM参数 jinfo 11666 jinfo -flags 11666 Xmx、Xms、Xmn、MetaspaceSize jstat：查看JVM运行时的状态信息，包括内存状态、垃圾回收 jstat [option] LVMID [interval] [count] 其中LVMID是进程id，interval是打印间隔时间（毫秒），count是打印次数（默认一直打印） option参数解释： -gc 垃圾回收堆的行为统计 -gccapacity 各个垃圾回收代容量(young,old,perm)和他们相应的空间统计 -gcutil 垃圾回收统计概述 -gcnew 新生代行为统计 -gcold 年老代和永生代行为统计 jstack：查看JVM线程快照，jstack命令可以定位线程出现长时间卡顿的原因，例如死锁，死循环 jstack [-l] &lt;pid&gt; (连接运行中的进程) option参数解释： -F 当使用jstack &lt;pid&gt;无响应时，强制输出线程堆栈。 -m 同时输出java和本地堆栈(混合模式) -l 额外显示锁信息 jmap：可以用来查看内存信息(配合jhat使用) jmap [option] &lt;pid&gt; (连接正在执行的进程) option参数解释： -heap 打印java heap摘要 -dump:&lt;dump-options&gt; 生成java堆的dump文件 5、JDK新特性JDK8 支持 Lamda 表达式、集合的 stream 操作、提升HashMap性能 JDK9 //Stream API中iterate方法的新重载方法，可以指定什么时候结束迭代 IntStream.iterate(1, i -&gt; i &lt; 100, i -&gt; i + 1).forEach(System.out::println); 默认G1垃圾回收器 JDK10 其重点在于通过完全GC并行来改善G1最坏情况的等待时间。 JDK11 ZGC (并发回收的策略) 4TB 用于 Lambda 参数的局部变量语法 JDK12 Shenandoah GC (GC 算法)停顿时间和堆的大小没有任何关系，并行关注停顿响应时间。 JDK13 增加ZGC以将未使用的堆内存返回给操作系统，16TB JDK14 删除cms垃圾回收器、弃用ParallelScavenge+SerialOldGC垃圾回收算法组合 将ZGC垃圾回收器应用到macOS和windows平台 线上故障排查1、硬件故障排查如果一个实例发生了问题，根据情况选择，要不要着急去重启。如果出现的CPU、内存飙高或者日志里出现了OOM异常 第一步是隔离，第二步是保留现场，第三步才是问题排查。 隔离 就是把你的这台机器从请求列表里摘除，比如把 nginx 相关的权重设成零。 现场保留 瞬时态和历史态 查看比如 CPU、系统内存等，通过历史状态可以体现一个趋势性问题，而这些信息的获取一般依靠监控系统的协作。 保留信息 （1）系统当前网络连接 ss -antp &gt; $DUMP_DIR/ss.dump 2&gt;&amp;1 使用 ss 命令而不是 netstat 的原因，是因为 netstat 在网络连接非常多的情况下，执行非常缓慢。 后续的处理，可通过查看各种网络连接状态的梳理，来排查 TIME_WAIT 或者 CLOSE_WAIT，或者其他连接过高的问题，非常有用。 （2）网络状态统计 netstat -s &gt; $DUMP_DIR/netstat-s.dump 2&gt;&amp;1 它能够按照各个协议进行统计输出，对把握当时整个网络状态，有非常大的作用。 sar -n DEV 1 2 &gt; $DUMP_DIR/sar-traffic.dump 2&gt;&amp;1 在一些速度非常高的模块上，比如 Redis、Kafka，就经常发生跑满网卡的情况。表现形式就是网络通信非常缓慢。 （3）进程资源 lsof -p $PID &gt; $DUMP_DIR/lsof-$PID.dump 通过查看进程，能看到打开了哪些文件，可以以进程的维度来查看整个资源的使用情况，包括每条网络连接、每个打开的文件句柄。同时，也可以很容易的看到连接到了哪些服务器、使用了哪些资源。这个命令在资源非常多的情况下，输出稍慢，请耐心等待。 （4）CPU 资源 mpstat &gt; $DUMP_DIR/mpstat.dump 2&gt;&amp;1 vmstat 1 3 &gt; $DUMP_DIR/vmstat.dump 2&gt;&amp;1 sar -p ALL &gt; $DUMP_DIR/sar-cpu.dump 2&gt;&amp;1 uptime &gt; $DUMP_DIR/uptime.dump 2&gt;&amp;1 主要用于输出当前系统的 CPU 和负载，便于事后排查。 （5）I/O 资源 iostat -x &gt; $DUMP_DIR/iostat.dump 2&gt;&amp;1 一般，以计算为主的服务节点，I/O 资源会比较正常，但有时也会发生问题，比如日志输出过多，或者磁盘问题等。此命令可以输出每块磁盘的基本性能信息，用来排查 I/O 问题。在第 8 课时介绍的 GC 日志分磁盘问题，就可以使用这个命令去发现。 （6）内存问题 free -h &gt; $DUMP_DIR/free.dump 2&gt;&amp;1 free 命令能够大体展现操作系统的内存概况，这是故障排查中一个非常重要的点，比如 SWAP 影响了 GC，SLAB 区挤占了 JVM 的内存。 （7）其他全局 ps -ef &gt; $DUMP_DIR/ps.dump 2&gt;&amp;1 dmesg &gt; $DUMP_DIR/dmesg.dump 2&gt;&amp;1 sysctl -a &gt; $DUMP_DIR/sysctl.dump 2&gt;&amp;1 dmesg 是许多静悄悄死掉的服务留下的最后一点线索。当然，ps 作为执行频率最高的一个命令，由于内核的配置参数，会对系统和 JVM 产生影响，所以我们也输出了一份。 （8）进程快照，最后的遗言（jinfo） $&#123;JDK_BIN&#125;jinfo $PID &gt; $DUMP_DIR/jinfo.dump 2&gt;&amp;1 此命令将输出 Java 的基本进程信息，包括环境变量和参数配置，可以查看是否因为一些错误的配置造成了 JVM 问题。 （9）dump 堆信息 $&#123;JDK_BIN&#125;jstat -gcutil $PID &gt; $DUMP_DIR/jstat-gcutil.dump 2&gt;&amp;1 $&#123;JDK_BIN&#125;jstat -gccapacity $PID &gt; $DUMP_DIR/jstat-gccapacity.dump 2&gt;&amp;1 jstat 将输出当前的 gc 信息。一般，基本能大体看出一个端倪，如果不能，可将借助 jmap 来进行分析。 （10）堆信息 $&#123;JDK_BIN&#125;jmap $PID &gt; $DUMP_DIR/jmap.dump 2&gt;&amp;1 $&#123;JDK_BIN&#125;jmap -heap $PID &gt; $DUMP_DIR/jmap-heap.dump 2&gt;&amp;1 $&#123;JDK_BIN&#125;jmap -histo $PID &gt; $DUMP_DIR/jmap-histo.dump 2&gt;&amp;1 $&#123;JDK_BIN&#125;jmap -dump:format=b,file=$DUMP_DIR/heap.bin $PID &gt; /dev/null 2&gt;&amp;1 jmap 将会得到当前 Java 进程的 dump 信息。如上所示，其实最有用的就是第 4 个命令，但是前面三个能够让你初步对系统概况进行大体判断。因为，第 4 个命令产生的文件，一般都非常的大。而且，需要下载下来，导入 MAT 这样的工具进行深入分析，才能获取结果。这是分析内存泄漏一个必经的过程。 （11）JVM 执行栈 $&#123;JDK_BIN&#125;jstack $PID &gt; $DUMP_DIR/jstack.dump 2&gt;&amp;1 jstack 将会获取当时的执行栈。一般会多次取值，我们这里取一次即可。这些信息非常有用，能够还原 Java 进程中的线程情况。 top -Hp $PID -b -n 1 -c &gt; $DUMP_DIR/top-$PID.dump 2&gt;&amp;1 为了能够得到更加精细的信息，我们使用 top 命令，来获取进程中所有线程的 CPU 信息，这样，就可以看到资源到底耗费在什么地方了。 （12）高级替补 kill -3 $PID 有时候，jstack 并不能够运行，有很多原因，比如 Java 进程几乎不响应了等之类的情况。我们会尝试向进程发送 kill -3 信号，这个信号将会打印 jstack 的 trace 信息到日志文件中，是 jstack 的一个替补方案。 gcore -o $DUMP_DIR/core $PID 对于 jmap 无法执行的问题，也有替补，那就是 GDB 组件中的 gcore，将会生成一个 core 文件。我们可以使用如下的命令去生成 dump： $&#123;JDK_BIN&#125;jhsdb jmap --exe $&#123;JDK&#125;java --core $DUMP_DIR/core --binaryheap 内存泄漏的现象 稍微提一下 jmap 命令，它在 9 版本里被干掉了，取而代之的是 jhsdb，你可以像下面的命令一样使用。 jhsdb jmap --heap --pid 37340 jhsdb jmap --pid 37288 jhsdb jmap --histo --pid 37340 jhsdb jmap --binaryheap --pid 37340 一般内存溢出，表现形式就是 Old 区的占用持续上升，即使经过了多轮 GC 也没有明显改善。比如ThreadLocal里面的GC Roots，内存泄漏的根本就是，这些对象并没有切断和 GC Roots 的关系，可通过一些工具，能够看到它们的联系。 2、报表异常 | JVM调优有一个报表系统，频繁发生内存溢出，在高峰期间使用时，还会频繁的发生拒绝服务，由于大多数使用者是管理员角色，所以很快就反馈到研发这里。 业务场景是由于有些结果集的字段不是太全，因此需要对结果集合进行循环，并通过 HttpClient 调用其他服务的接口进行数据填充。使用 Guava 做了 JVM 内缓存，但是响应时间依然很长。 初步排查，JVM 的资源太少。接口 A 每次进行报表计算时，都要涉及几百兆的内存，而且在内存里驻留很长时间，有些计算又非常耗 CPU，特别的“吃”资源。而我们分配给 JVM 的内存只有 3 GB，在多人访问这些接口的时候，内存就不够用了，进而发生了 OOM。在这种情况下，没办法，只有升级机器。把机器配置升级到 4C8G，给 JVM 分配 6GB 的内存，这样 OOM 问题就消失了。但随之而来的是频繁的 GC 问题和超长的 GC 时间，平均 GC 时间竟然有 5 秒多。 进一步，由于报表系统和高并发系统不太一样，它的对象，存活时长大得多，并不能仅仅通过增加年轻代来解决；而且，如果增加了年轻代，那么必然减少了老年代的大小，由于 CMS 的碎片和浮动垃圾问题，我们可用的空间就更少了。虽然服务能够满足目前的需求，但还有一些不太确定的风险。 第一，了解到程序中有很多缓存数据和静态统计数据，为了减少 MinorGC 的次数，通过分析 GC 日志打印的对象年龄分布，把 MaxTenuringThreshold 参数调整到了 3（特殊场景特殊的配置）。这个参数是让年轻代的这些对象，赶紧回到老年代去，不要老呆在年轻代里。 第二，我们的 GC 时间比较长，就一块开了参数 CMSScavengeBeforeRemark，使得在 CMS remark 前，先执行一次 Minor GC 将新生代清掉。同时配合上个参数，其效果还是比较好的，一方面，对象很快晋升到了老年代，另一方面，年轻代的对象在这种情况下是有限的，在整个 MajorGC 中占的时间也有限。 第三，由于缓存的使用，有大量的弱引用，拿一次长达 10 秒的 GC 来说。我们发现在 GC 日志里，处理 weak refs 的时间较长，达到了 4.5 秒。这里可以加入参数 ParallelRefProcEnabled 来并行处理Reference，以加快处理速度，缩短耗时。 优化之后，效果不错，但并不是特别明显。经过评估，针对高峰时期的情况进行调研，我们决定再次提升机器性能，改用 8core16g 的机器。但是，这带来另外一个问题。 高性能的机器带来了非常大的服务吞吐量，通过 jstat 进行监控，能够看到年轻代的分配速率明显提高，但随之而来的 MinorGC 时长却变的不可控，有时候会超过 1 秒。累积的请求造成了更加严重的后果。 这是由于堆空间明显加大造成的回收时间加长。为了获取较小的停顿时间，我们在堆上改用了 G1 垃圾回收器，把它的目标设定在 200ms。G1 是一款非常优秀的垃圾收集器，不仅适合堆内存大的应用，同时也简化了调优的工作。通过主要的参数初始和最大堆空间、以及最大容忍的 GC 暂停目标，就能得到不错的性能。修改之后，虽然 GC 更加频繁了一些，但是停顿时间都比较小，应用的运行较为平滑。 到目前为止，也只是勉强顶住了已有的业务，但是，这时候领导层面又发力，要求报表系统可以支持未来两年业务10到100倍的增长，并保持其可用性，但是这个“千疮百孔”的报表系统，稍微一压测，就宕机，那如何应对十倍百倍的压力呢 ? 硬件即使可以做到动态扩容，但是毕竟也有极限。 使用 MAT 分析堆快照，发现很多地方可以通过代码优化，那些占用内存特别多的对象： 1、select * 全量排查，只允许获取必须的数据 2、报表系统中cache实际的命中率并不高，将Guava 的 Cache 引用级别改成弱引用（WeakKeys） 3、限制报表导入文件大小，同时拆分用户超大范围查询导出请求。 每一步操作都使得JVM使用变得更加可用，一系列优化以后，机器相同压测数据性能提升了数倍。 3、大屏异常 | JUC调优有些数据需要使用 HttpClient 来获取进行补全。提供数据的服务提供商有的响应时间可能会很长，也有可能会造成服务整体的阻塞。 接口 A 通过 HttpClient 访问服务 2，响应 100ms 后返回；接口 B 访问服务 3，耗时 2 秒。HttpClient 本身是有一个最大连接数限制的，如果服务 3 迟迟不返回，就会造成 HttpClient 的连接数达到上限，概括来讲，就是同一服务，由于一个耗时非常长的接口，进而引起了整体的服务不可用 这个时候，通过 jstack 打印栈信息，会发现大多数竟然阻塞在了接口 A 上，而不是耗时更长的接口 B，这个现象起初十分具有迷惑性，不过经过分析后，我们猜想其实是因为接口 A 的速度比较快，在问题发生点进入了更多的请求，它们全部都阻塞住的同时被打印出来了。 为了验证这个问题，我搭建了一个demo 工程，模拟了两个使用同一个 HttpClient 的接口。fast 接口用来访问百度，很快就能返回；slow 接口访问谷歌，由于众所周知的原因，会阻塞直到超时，大约 10 s。 利用ab对两个接口进行压测，同时使用 jstack 工具 dump 堆栈。首先使用 jps 命令找到进程号，然后把结果重定向到文件（可以参考 10271.jstack 文件）。 过滤一下 nio 关键字，可以查看 tomcat 相关的线程，足足有 200 个，这和 Spring Boot 默认的 maxThreads 个数不谋而合。更要命的是，有大多数线程，都处于 BLOCKED 状态，说明线程等待资源超时。通过grep fast | wc -l 分析，确实200个中有150个都是blocked的fast的进程。 问题找到了，解决方式就顺利成章了。 1、fast和slow争抢连接资源，通过线程池限流或者熔断处理 2、有时候slow的线程也不是一直slow，所以就得加入监控 3、使用带countdownLaunch对线程的执行顺序逻辑进行控制 4、接口延迟 | SWAP调优有一个关于服务的某个实例，经常发生服务卡顿。由于服务的并发量是比较高的，每多停顿 1 秒钟，几万用户的请求就会感到延迟。 我们统计、类比了此服务其他实例的 CPU、内存、网络、I/O 资源，区别并不是很大，所以一度怀疑是机器硬件的问题。 接下来我们对比了节点的 GC 日志，发现无论是 Minor GC，还是 Major GC，这个节点所花费的时间，都比其他实例长得多。 通过仔细观察，我们发现在 GC 发生的时候，vmstat 的 si、so 飙升的非常严重，这和其他实例有着明显的不同。 使用 free 命令再次确认，发现 SWAP 分区，使用的比例非常高，引起的具体原因是什么呢？ 更详细的操作系统内存分布，从 /proc/meminfo 文件中可以看到具体的逻辑内存块大小，有多达 40 项的内存信息，这些信息都可以通过遍历 /proc 目录的一些文件获取。我们注意到 slabtop 命令显示的有一些异常，dentry（目录高速缓冲）占用非常高。 问题最终定位到是由于某个运维工程师删除日志时，定时执行了一句命令： find / | grep “xxx.log” 他是想找一个叫做 要被删除 的日志文件，看看在哪台服务器上，结果，这些老服务器由于文件太多，扫描后这些文件信息都缓存到了 slab 区上。而服务器开了 swap，操作系统发现物理内存占满后，并没有立即释放 cache，导致每次 GC 都要和硬盘打一次交道。 解决方式就是关闭 SWAP 分区。 swap 是很多性能场景的万恶之源，建议禁用。在高并发 SWAP 绝对能让你体验到它魔鬼性的一面：进程倒是死不了了，但 GC 时间长的却让人无法忍受。 5、内存溢出 | Cache调优 有一次线上遇到故障，重新启动后，使用 jstat 命令，发现 Old 区一直在增长。我使用 jmap 命令，导出了一份线上堆栈，然后使用 MAT 进行分析，通过对 GC Roots 的分析，发现了一个非常大的 HashMap 对象，这个原本是其他同事做缓存用的，但是做了一个无界缓存，没有设置超时时间或者 LRU 策略，在使用上又没有重写key类对象的hashcode和equals方法，对象无法取出也直接造成了堆内存占用一直上升，后来，将这个缓存改成 guava 的 Cache，并设置了弱引用，故障就消失了。 关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally 块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。 内存溢出是一个结果，而内存泄漏是一个原因。内存溢出的原因有内存空间不足、配置错误等因素。一些错误的编程方式，不再被使用的对象、没有被回收、没有及时切断与 GC Roots 的联系，这就是内存泄漏。 举个例子，有团队使用了 HashMap 做缓存，但是并没有设置超时时间或者 LRU 策略，造成了放入 Map 对象的数据越来越多，而产生了内存泄漏。 再来看一个经常发生的内存泄漏的例子，也是由于 HashMap 产生的。代码如下，由于没有重写 Key 类的 hashCode 和 equals 方法，造成了放入 HashMap 的所有对象都无法被取出来，它们和外界失联了。所以下面的代码结果是 null。 //leak example import java.util.HashMap; import java.util.Map; public class HashMapLeakDemo &#123; public static class Key &#123; String title; public Key(String title) &#123; this.title = title; &#125; &#125; public static void main(String[] args) &#123; Map&lt;Key, Integer&gt; map = new HashMap&lt;&gt;(); map.put(new Key(&quot;1&quot;), 1); map.put(new Key(&quot;2&quot;), 2); map.put(new Key(&quot;3&quot;), 2); Integer integer = map.get(new Key(&quot;2&quot;)); System.out.println(integer); &#125; &#125; 即使提供了 equals 方法和 hashCode 方法，也要非常小心，尽量避免使用自定义的对象作为 Key。 再看一个例子，关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally 块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。 6：CPU飙高 | 死循环我们有个线上应用，单节点在运行一段时间后，CPU 的使用会飙升，一旦飙升，一般怀疑某个业务逻辑的计算量太大，或者是触发了死循环（比如著名的 HashMap 高并发引起的死循环），但排查到最后其实是 GC 的问题。 （1）使用 top 命令，查找到使用 CPU 最多的某个进程，记录它的 pid。使用 Shift + P 快捷键可以按 CPU 的使用率进行排序。 top （2）再次使用 top 命令，加 -H 参数，查看某个进程中使用 CPU 最多的某个线程，记录线程的 ID。 top -Hp $pid （3）使用 printf 函数，将十进制的 tid 转化成十六进制。 printf %x $tid （4）使用 jstack 命令，查看 Java 进程的线程栈。 jstack $pid &gt;$pid.log （5）使用 less 命令查看生成的文件，并查找刚才转化的十六进制 tid，找到发生问题的线程上下文。 less $pid.log 我们在 jstack 日志搜关键字DEAD，以及中找到了 CPU 使用最多的几个线程id。 可以看到问题发生的根源，是我们的堆已经满了，但是又没有发生 OOM，于是 GC 进程就一直在那里回收，回收的效果又非常一般，造成 CPU 升高应用假死。接下来的具体问题排查，就需要把内存 dump 一份下来，使用 MAT 等工具分析具体原因了。 三、多线程篇线程调度1、线程状态​ 线程是cpu任务调度的最小执行单位，每个线程拥有自己独立的程序计数器、虚拟机栈、本地方法栈 线程状态：创建、就绪、运行、阻塞、死亡 2、线程状态切换 方法 作用 区别 start 启动线程，由虚拟机自动调度执行run()方法 线程处于就绪状态 run 线程逻辑代码块处理，JVM调度执行 线程处于运行状态 sleep 让当前正在执行的线程休眠（暂停执行） 不释放锁 wait 使得当前线程等待 释放同步锁 notify 唤醒在此对象监视器上等待的单个线程 唤醒单个线程 notifyAll 唤醒在此对象监视器上等待的所有线程 唤醒多个线程 yiled 停止当前线程，让同等优先权的线程运行 用Thread类调用 join 使当前线程停下来等待，直至另一个调用join方法的线程终止 用线程对象调用 3、阻塞唤醒过程阻塞： ​ 这三个方法的调用都会使当前线程阻塞。该线程将会被放置到对该Object的请求等待队列中，然后让出当前对Object所拥有的所有的同步请求。线程会一直暂停所有线程调度，直到下面其中一种情况发生： ① 其他线程调用了该Object的notify方法，而该线程刚好是那个被唤醒的线程； ② 其他线程调用了该Object的notifyAll方法； 唤醒： ​ 线程将会从等待队列中移除，重新成为可调度线程。它会与其他线程以常规的方式竞争对象同步请求。一旦它重新获得对象的同步请求，所有之前的请求状态都会恢复，也就是线程调用wait的地方的状态。线程将会在之前调用wait的地方继续运行下去。 为什么要出现在同步代码块中： ​ 由于wait()属于Object方法，调用之后会强制释放当前对象锁，所以在wait() 调用时必须拿到当前对象的监视器monitor对象。因此，wait()方法在同步方法/代码块中调用。 4、wait和sleep区别 wait 方法必须在 synchronized 保护的代码中使用，而 sleep 方法并没有这个要求。 wait 方法会主动释放 monitor 锁，在同步代码中执行 sleep 方法时，并不会释放 monitor 锁。 wait 方法意味着永久等待，直到被中断或被唤醒才能恢复，不会主动恢复，sleep 方法中会定义一个时间，时间到期后会主动恢复。 wait/notify 是 Object 类的方法，而 sleep 是 Thread 类的方法。 5、创建线程方式实现 Runnable 接口（优先使用） public class RunnableThread implements Runnable &#123; @Override public void run() &#123;System.out.println(&#39;用实现Runnable接口实现线程&#39;);&#125; &#125; 实现Callable接口（有返回值可抛出异常） class CallableTask implements Callable&lt;Integer&gt; &#123; @Override public Integer call() throws Exception &#123; return new Random().nextInt();&#125; &#125; 继承Thread类（java不支持多继承） public class ExtendsThread extends Thread &#123; @Override public void run() &#123;System.out.println(&#39;用Thread类实现线程&#39;);&#125; &#125; 使用线程池（底层都是实现run方法） static class DefaultThreadFactory implements ThreadFactory &#123; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = &quot;pool-&quot; + poolNumber.getAndIncrement() +&quot;-thread-&quot;; &#125; public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r,namePrefix + threadNumber.getAndIncrement(),0); if (t.isDaemon()) t.setDaemon(false); //是否守护线程 if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); //线程优先级 return t; &#125; &#125; 线程池优点：通过复用已创建的线程，降低资源损耗、线程可以直接处理队列中的任务加快响应速度、同时便于统一监控和管理。 1、线程池构造函数/** * 线程池构造函数7大参数 */ public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime, TimeUnit unit,BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 参数介绍： 参数 作用 corePoolSize 核心线程池大小 maximumPoolSize 最大线程池大小 keepAliveTime 线程池中超过 corePoolSize 数目的空闲线程最大存活时间； TimeUnit keepAliveTime 时间单位 workQueue 阻塞任务队列 threadFactory 新建线程工厂 RejectedExecutionHandler 拒绝策略。当提交任务数超过 maxmumPoolSize+workQueue 之和时，任务会交给RejectedExecutionHandler 来处理 2、线程处理任务过程： 当线程池小于corePoolSize，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程。 当线程池达到corePoolSize时，新提交任务将被放入 workQueue 中，等待线程池中任务调度执行。 当workQueue已满，且 maximumPoolSize 大于 corePoolSize 时，新提交任务会创建新线程执行任务。 当提交任务数超过 maximumPoolSize 时，新提交任务由 RejectedExecutionHandler 处理。 当线程池中超过corePoolSize 线程，空闲时间达到 keepAliveTime 时，关闭空闲线程 。 3、线程拒绝策略​ 线程池中的线程已经用完了，无法继续为新任务服务，同时，等待队列也已经排满了，再也塞不下新任务了。这时候我们就需要拒绝策略机制合理的处理这个问题。 JDK 内置的拒绝策略如下： ​ AbortPolicy：直接抛出异常，阻止系统正常运行。可以根据业务逻辑选择重试或者放弃提交等策略。 ​ CallerRunsPolicy ：只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。 ​ 不会造成任务丢失，同时减缓提交任务的速度，给执行任务缓冲时间。 ​ DiscardOldestPolicy ：丢弃最老的一个请求，也就是即将被执行的任务，并尝试再次提交当前任务。 ​ DiscardPolicy ：该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案。 4、Execuors类实现线程池 newSingleThreadExecutor()：只有一个线程的线程池，任务是顺序执行，适用于一个一个任务执行的场景 newCachedThreadPool()：线程池里有很多线程需要同时执行，60s内复用，适用执行很多短期异步的小程序或者负载较轻的服务 newFixedThreadPool()：拥有固定线程数的线程池，如果没有任务执行，那么线程会一直等待，适用执行长期的任务。 newScheduledThreadPool()：用来调度即将执行的任务的线程池 **newWorkStealingPool()**：底层采用forkjoin的Deque，采用独立的任务队列可以减少竞争同时加快任务处理 因为以上方式都存在弊端： ​ FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列⻓度为 Integer.MAX_VALUE，会导致OOM。​ CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE，会导致OOM。 手动创建的线程池底层使用的是ArrayBlockingQueue可以防止OOM。 5、线程池大小设置 CPU 密集型（n+1） ​ CPU 密集的意思是该任务需要大量的运算，而没有阻塞，CPU 一直全速运行。 ​ CPU 密集型任务尽可能的少的线程数量，一般为 CPU 核数 + 1 个线程的线程池。 IO 密集型（2*n） ​ 由于 IO 密集型任务线程并不是一直在执行任务，可以多分配一点线程数，如 CPU * 2 ​ 也可以使用公式：CPU 核心数 *（1+平均等待时间/平均工作时间）。 线程安全1、乐观锁，CAS思想java乐观锁机制： ​ 乐观锁体现的是悲观锁的反面。它是一种积极的思想，它总是认为数据是不会被修改的，所以是不会对数据上锁的。但是乐观锁在更新的时候会去判断数据是否被更新过。乐观锁的实现方案一般有两种（版本号机制和CAS）。乐观锁适用于读多写少的场景，这样可以提高系统的并发量。在Java中 java.util.concurrent.atomic下的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 乐观锁，大多是基于数据版本 (Version)记录机制实现。即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来 实现。 读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提 交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据 版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 CAS思想： ​ CAS就是compare and swap（比较交换），是一种很出名的无锁的算法，就是可以不使用锁机制实现线程间的同步。使用CAS线程是不会被阻塞的，所以又称为非阻塞同步。CAS算法涉及到三个操作： ​ 需要读写内存值V；进行比较的值A；准备写入的值B ​ 当且仅当V的值等于A的值等于V的值的时候，才用B的值去更新V的值，否则不会执行任何操作（比较和替换是一个原子操作-A和V比较，V和B替换），一般情况下是一个自旋操作，即不断重试 缺点： ​ ABA问题-知乎 ​ 高并发的情况下，很容易发生并发冲突，如果CAS一直失败，那么就会一直重试，浪费CPU资源 原子性： ​ 功能限制CAS是能保证单个变量的操作是原子性的，在Java中要配合使用volatile关键字来保证线程的安全；当涉及到多个变量的时候CAS无能为力；除此之外CAS实现需要硬件层面的支持，在Java的普通用户中无法直接使用，只能借助atomic包下的原子类实现，灵活性受到了限制 2、synchronized底层实现使用方法：主要的三种使⽤⽅式 ​ 修饰实例⽅法: 作⽤于当前对象实例加锁，进⼊同步代码前要获得当前对象实例的锁 ​ 修饰静态⽅法: 也就是给当前类加锁，会作⽤于类的所有对象实例，因为静态成员不属于任何⼀个实例对象，是类成员。 ​ 修饰代码块: 指定加锁对象，对给定对象加锁，进⼊同步代码库前要获得给定对象的锁。 ​ 总结：synchronized锁住的资源只有两类：一个是对象，一个是类。 底层实现： ​ 对象头是我们需要关注的重点，它是synchronized实现锁的基础，因为synchronized申请锁、上锁、释放锁都与对象头有关。对象头主要结构是由Mark Word 组成，其中Mark Word存储对象的hashCode、锁信息或分代年龄或GC标志等信息。 ​ 锁也分不同状态，JDK6之前只有两个状态：无锁、有锁（重量级锁），而在JDK6之后对synchronized进行了优化，新增了两种状态，总共就是四个状态：无锁状态、偏向锁、轻量级锁、重量级锁，其中无锁就是一种状态了。锁的类型和状态在对象头Mark Word中都有记录，在申请锁、锁升级等过程中JVM都需要读取对象的Mark Word数据。 ​ 同步代码块是利用 monitorenter 和 monitorexit 指令实现的，而同步方法则是利用 flags 实现的。 3、ReenTrantLock底层实现​ 由于ReentrantLock是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能 使用方法： ​ 基于API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成 底层实现： ​ ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 和synchronized区别： ​ 1、底层实现：synchronized 是JVM层面的锁，是Java关键字，通过monitor对象来完成（monitorenter与monitorexit），ReentrantLock 是从jdk1.5以来（java.util.concurrent.locks.Lock）提供的API层面的锁。 ​ 2、实现原理****：synchronized 的实现涉及到锁的升级，具体为无锁、偏向锁、自旋锁、向OS申请重量级锁；ReentrantLock实现则是通过利用CAS**（CompareAndSwap）自旋机制保证线程操作的原子性和volatile保证数据可见性以实现锁的功能。 ​ 3、是否可手动释放：synchronized 不需要用户去手动释放锁，synchronized 代码执行完后系统会自动让线程释放对锁的占用； ReentrantLock则需要用户去手动释放锁，如果没有手动释放锁，就可能导致死锁现象。 ​ 4、是否可中断synchronized是不可中断类型的锁，除非加锁的代码中出现异常或正常执行完成； ReentrantLock则可以中断，可通过trylock(long timeout,TimeUnit unit)设置超时方法或者将lockInterruptibly()放到代码块中，调用interrupt方法进行中断。 ​ 5、是否公平锁synchronized为非公平锁 ReentrantLock则即可以选公平锁也可以选非公平锁，通过构造方法new ReentrantLock时传入boolean值进行选择，为空默认false非公平锁，true为公平锁,公平锁性能非常低。 4、公平锁和非公平锁区别公平锁： ​ 公平锁自然是遵循FIFO（先进先出）原则的，先到的线程会优先获取资源，后到的会进行排队等待 ​ 优点：所有的线程都能得到资源，不会饿死在队列中。适合大任务 ​ 缺点：吞吐量会下降，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销大 非公平锁： ​ 多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。 ​ 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。 ​ 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁 公平锁效率低原因： ​ 公平锁要维护一个队列，后来的线程要加锁，即使锁空闲，也要先检查有没有其他线程在 wait，如果有自己要挂起，加到队列后面，然后唤醒队列最前面线程。这种情况下相比较非公平锁多了一次挂起和唤醒。 ​ 线程切换的开销，其实就是非公平锁效率高于公平锁的原因，因为非公平锁减少了线程挂起的几率，后来的线程有一定几率逃离被挂起的开销。 5、使用层面锁优化​ 【1】减少锁的时间：​ 不需要同步执行的代码，能不放在同步快里面执行就不要放在同步快内，可以让锁尽快释放； ​ 【2】减少锁的粒度：​ 它的思想是将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间；java中很多数据结构都是采用这种方法提高并发操作的效率，比如： ​ ConcurrentHashMap： ​ java中的ConcurrentHashMap在jdk1.8之前的版本，使用一个Segment 数组：Segment&lt; K,V &gt;[] segments ​ Segment继承自ReenTrantLock，所以每个Segment是个可重入锁，每个Segment 有一个HashEntry&lt; K,V &gt;数组用来存放数据，put操作时，先确定往哪个Segment放数据，只需要锁定这个Segment，执行put，其它的Segment不会被锁定；所以数组中有多少个Segment就允许同一时刻多少个线程存放数据，这样增加了并发能力。 ​ 【3】锁粗化：​ 大部分情况下我们是要让锁的粒度最小化，锁的粗化则是要增大锁的粒度; ​ 假如有一个循环，循环内的操作需要加锁，我们应该把锁放到循环外面，否则每次进出循环，都进出一次临界区，效率是非常差的； ​ 【4】使用读写锁： ​ ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可并发读，写操作使用写锁，只能单线程写； ​ 【5】使用CAS： ​ 如果需要同步的操作执行速度非常快，并且线程竞争并不激烈，这时候使用cas效率会更高，因为加锁会导致线程的上下文切换，如果上下文切换的耗时比同步操作本身更耗时，且线程对资源的竞争不激烈，使用volatiled+cas操作会是非常高效的选择； 6、系统层面锁优化自适应自旋锁： ​ 自旋锁可以避免等待竞争锁进入阻塞挂起状态被唤醒造成的内核态和用户态之间的切换的损耗，它们只需要等一等（自旋），但是如果锁被其他线程长时间占用，一直不释放CPU，死等会带来更多的性能开销；自旋次数默认值是10 ​ 对上面自旋锁优化方式的进一步优化，它的自旋的次数不再固定，其自旋的次数由前一次在同一个锁上的自旋时间及锁的拥有者的状态